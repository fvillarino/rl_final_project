{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "027229e4-9c75-4418-9111-3d1afc4c5526",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d47e401-bb8b-4200-8aa9-afaa856a924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from boardgame2 import ReversiEnv\n",
    "import numpy as np\n",
    "from players import RandomPlayer\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74717cb-c130-4894-99ec-548d0f39acf3",
   "metadata": {},
   "source": [
    "# SelfPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3f09c2-5a93-4a5e-9e53-a11391af1cea",
   "metadata": {},
   "source": [
    "gym.spaces.Box(-1, 1, (1, board_shape,board_shape))En esta notebook se pide armar un entorno al cual se le pase como parámetro la clase de jugador local (DictPlayer, RandomPlayer, GreedyPlayer), y que el entorno devuelva el siguiente paso luego de jugar con el jugador local. Algunas condiciones:\n",
    "- En la función de reset(), se sortearea si el jugador local juega primero o segundo. \n",
    "- El entorno siempre devolverá el tablero como si le tocará jugar al jugador 1. Sea primero o segundo\n",
    "- La clase se instancia con los siguientes parámetros:\n",
    "    - LocalPlayer\n",
    "    - board_shape\n",
    "    \n",
    "- El método step recibirá como parámtro la acción pero codificada no como action = [columna, fila], si no como: action = action[0] * board_shape + action[1]\n",
    "- self.action_space tiene que estar definido acorde al espacio de acción. Por ejemplo: self.action_space = gym.spaces.Discrete(board_shape**2)\n",
    "- self.observation_space también: self.observation_space = gym.spaces.Box(-1, 1, (1, board_shape,board_shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71ee1121-2da1-4d55-b02c-b1c37c07d8f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-1.0, 1.0, (1, 8, 8), float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.spaces.Box(-1, 1, (1, 8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1d97d4-db5e-46b7-a273-4fad6bc07373",
   "metadata": {},
   "source": [
    "# Ejemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03553fc6-987c-4e88-9cf3-777cd4b87843",
   "metadata": {},
   "source": [
    "El jugador local juega segundo entonces el reset() devuelve (Notar que no se devuelve el player por que siempre juega el 1):"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a50ac17-cfa0-452b-9fa2-63a495ffbe6f",
   "metadata": {},
   "source": [
    "[[ 0,  0,  0,  0],\n",
    " [ 0,  1, -1,  0],\n",
    " [ 0, -1,  1,  0],\n",
    " [ 0,  0,  0,  0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3090f893-6ede-4b8d-9559-b01341181dee",
   "metadata": {},
   "source": [
    "El jugador local juega primero entonces el reset() devuelve:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7cfa1d6c-c465-46a5-b539-56edd8b6513c",
   "metadata": {},
   "source": [
    "[[ 0,  0, -1,  0],\n",
    " [ 0, -1, -1,  0],\n",
    " [ 0,  1, -1,  0],\n",
    " [ 0,  0,  0,  0]]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac37d6d6-e177-4cc7-b1f3-f136e6bb40fd",
   "metadata": {},
   "source": [
    "Que ocurrio aca?\n",
    "\n",
    "El tablero se resetea y queda:\n",
    "\n",
    "[[ 0,  0,  0,  0],\n",
    " [ 0,  1, -1,  0],\n",
    " [ 0, -1,  1,  0],\n",
    " [ 0,  0,  0,  0]]\n",
    " \n",
    "Luego el jugador local muestrea una de las cuatros opciones válidas y juega (0, 2) comiendo la pieza (1, 2) y tranformandola en 1\n",
    "\n",
    "[[ 0,  0,  1,  0],\n",
    " [ 0,  1,  1,  0],\n",
    " [ 0, -1,  1,  0],\n",
    " [ 0,  0,  0,  0]]\n",
    " \n",
    "Ahora le toca el turno al jugador -1 pero el jugador externo tiene que ver el tablero como si fuera el 1, entonces se multiplica el tablero por -1\n",
    "\n",
    "[[ 0,  0, -1,  0],\n",
    " [ 0, -1, -1,  0],\n",
    " [ 0,  1, -1,  0],\n",
    " [ 0,  0,  0,  0]]\n",
    " \n",
    "Ahora el jugador externo seleccionará una acción observando el tablero como si fuera 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f918a12-4d72-48cb-84b0-617054ec02a3",
   "metadata": {},
   "source": [
    "gym.spaces.Discrete(board_shape**2)En cuanto a la recompenza tener en cuenta que deberá devolver:\n",
    "- 1 si gana el jugador externo\n",
    "- -1 si gana el LocalPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bef1f596-1a0e-449f-86f1-5c5115a00d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfPlayEnv(ReversiEnv):\n",
    "    def __init__(self, board_shape=8, LocalPlayer=None):\n",
    "        self.players = [-1, 1]\n",
    "\n",
    "        self.local_player = LocalPlayer(board_shape=board_shape, flatten_action=False)\n",
    "        self.board_shape = board_shape\n",
    "        super(SelfPlayEnv, self).__init__(board_shape=board_shape)\n",
    "        \n",
    "        self.action_space = gym.spaces.Discrete(board_shape**2)# Implementar\n",
    "        self.observation_space = gym.spaces.Box(-1, 1, (1, board_shape,board_shape))# Implementar\n",
    "         \n",
    "        \n",
    "    def play(self, observation):\n",
    "        action = self.local_player.predict(observation)\n",
    "        (observation, self.current_player_num), reward, done, info = super(SelfPlayEnv, self).step(action)\n",
    "\n",
    "        return (observation, self.current_player_num), reward, done, info\n",
    "    \n",
    "    def encode_observation(self, observation, valid_actions=False):\n",
    "        # Implementar\n",
    "        # Simpre devuelve desde el punto de vista del jugador 1\n",
    "        # No devuleve el jugador sino solo el tablero\n",
    "        # Tener en cuenta que esto será la entrada a la red neuronal\n",
    "        #observation = observation * self.player\n",
    "        #return observation\n",
    "        board = observation * self.current_player_num\n",
    "        if valid_actions:\n",
    "            return np.array([board, self.get_valid((board, 1))])\n",
    "        else:\n",
    "            return observation * self.current_player_num\n",
    "      \n",
    "    \n",
    "    def reset(self):\n",
    "        self.n_step = 0\n",
    "        self.local_player_num = np.random.choice(self.players)\n",
    "        self.local_player.player = self.local_player_num\n",
    "        self.observation, self.current_player_num = super(SelfPlayEnv, self).reset()\n",
    "        self.allow_pass = True\n",
    "            \n",
    "        if self.current_player_num == self.local_player_num:   \n",
    "            (self.observation, self.current_player_num), _, done, info = self.play(self.observation)\n",
    "            assert done == False\n",
    "\n",
    "        \n",
    "        return self.encode_observation(self.observation)\n",
    "    \n",
    "    def encode_action(self, action):\n",
    "        # Esta es necesario ya que la salida de la red neuronal será un valor entre 0 y board_shape**2 - 1\n",
    "        return [action // self.board_shape, action % self.board_shape]\n",
    "    \n",
    "    def decode_action(self, action):\n",
    "        return action[0] * self.board_shape + action[1]\n",
    "\n",
    "    def step(self, action):\n",
    "        self.n_step += 1\n",
    "        action = self.encode_action(action)\n",
    "        (self.observation, self.current_player_num), reward, done, _ = super(SelfPlayEnv, self).step(action)   \n",
    "        #print(f'action {action} - Done: {done}')\n",
    "        while not done and (self.current_player_num == self.local_player_num):            \n",
    "            #print(f'Jugó {self.local_player}')\n",
    "            (self.observation, self.current_player_num), reward, done, info = self.play(self.observation)\n",
    "        \n",
    "        encoded_observation = self.encode_observation(self.observation)\n",
    "        \n",
    "        #reward = float(encoded_observation.sum())# Implementar. Tiene que ser un float\n",
    "        reward = -float(self.local_player_num*reward)\n",
    "        #print(f'reward: {reward}')\n",
    "        return encoded_observation, reward, done, {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1ce3799d-d2bd-4590-b076-25e742c235f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SelfPlayEnv(board_shape=4, LocalPlayer=RandomPlayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bab4a7bd-0b0c-47e4-a63f-465fa83226de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0],\n",
       "       [ 0,  1, -1,  0],\n",
       "       [ 0, -1,  1,  0],\n",
       "       [ 0,  0,  0,  0]], dtype=int8)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0c2352-7ff1-455c-b2e8-b0a9844dd1f7",
   "metadata": {},
   "source": [
    "# Probar entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5d80054a-c061-4973-9a95-ab76fa91a693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_valid_actions(state):\n",
    "    # np.argwhere junto con env.get_valid y randint solucionan el problema en pocas lineas pero puede usar otra estrategia\n",
    "    board_shape = state.shape[0]\n",
    "    # El player es siempre 1\n",
    "    player = 1\n",
    "    valid_actions = np.argwhere(env.get_valid((state, player)) == 1)\n",
    "    action = valid_actions[np.random.randint(len(valid_actions))]\n",
    "    return action[0] * board_shape + action[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "af7b2ed4-3603-46e7-8cf9-57ed632627ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 1, 0]], dtype=int8)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_valid((env.board, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f074437d-a34d-4bdf-a122-164c1ee63123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0]\n",
      " [ 0  1 -1  0]\n",
      " [ 0 -1  1  0]\n",
      " [ 0  0  0  0]]\n",
      "acion acá: 13\n",
      "action [3, 1] - Done: False\n",
      "Jugó <players.RandomPlayer object at 0x7fec50bb31c0>\n",
      "reward: 0.0\n",
      "[[ 0  0  0  0]\n",
      " [-1 -1 -1  0]\n",
      " [ 0  1  1  0]\n",
      " [ 0  1  0  0]]\n",
      "acion acá: 0\n",
      "action [0, 0] - Done: False\n",
      "Jugó <players.RandomPlayer object at 0x7fec50bb31c0>\n",
      "reward: 0.0\n",
      "[[ 1  0  0  0]\n",
      " [-1  1 -1  0]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  1 -1  0]]\n",
      "acion acá: 7\n",
      "action [1, 3] - Done: False\n",
      "Jugó <players.RandomPlayer object at 0x7fec50bb31c0>\n",
      "reward: 0.0\n",
      "[[ 1  0 -1  0]\n",
      " [-1  1 -1  1]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0  1 -1  0]]\n",
      "acion acá: 15\n",
      "action [3, 3] - Done: False\n",
      "Jugó <players.RandomPlayer object at 0x7fec50bb31c0>\n",
      "reward: 0.0\n",
      "[[ 1 -1 -1  0]\n",
      " [-1 -1 -1  1]\n",
      " [ 0 -1  1  0]\n",
      " [ 0  1  1  1]]\n",
      "acion acá: 8\n",
      "action [2, 0] - Done: False\n",
      "Jugó <players.RandomPlayer object at 0x7fec50bb31c0>\n",
      "reward: 0.0\n",
      "[[ 1 -1 -1  0]\n",
      " [ 1 -1 -1  1]\n",
      " [ 1 -1  1  0]\n",
      " [-1  1  1  1]]\n",
      "acion acá: 3\n",
      "action [0, 3] - Done: False\n",
      "Jugó <players.RandomPlayer object at 0x7fec50bb31c0>\n",
      "reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "board = env.reset()\n",
    "while not done:\n",
    "    action = sample_valid_actions(board)\n",
    "    print(board)\n",
    "    print(f'acion acá: {action}')\n",
    "    \n",
    "    board, reward, done, _ = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cc2a60-1484-43cf-9cfe-3bc435aaa548",
   "metadata": {},
   "source": [
    "# Entornos vectoriales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "0ce9342c-4c16-4fa2-aed7-a591e4fad79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_env import make_reversi_vec_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "7dfb931b-9602-489f-b52c-4266b054b78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "board_shape = 8\n",
    "n_envs = 10\n",
    "env = make_reversi_vec_env(\n",
    "    SelfPlayEnv, n_envs=n_envs,\n",
    "    env_kwargs={\n",
    "        'board_shape': board_shape,\n",
    "        'LocalPlayer': RandomPlayer\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "fb45e843-724f-4318-942e-7b581909d973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1, 8, 8)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6172549d-264f-4e31-bba9-939f5a1cd61d",
   "metadata": {},
   "source": [
    "- Notar que la entrada tiene como primer componente la cantidad de entornos en paralelo (10), luego la cantidad de canales (1), y finalmente las dimensiones del tablero \n",
    "\n",
    "- Imprimir obs y ver que hay distintas posibles entradas dependiendo de quien juega primero y que jugó el LocalPlayer si le toco primero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "3ae152cc-bdb5-4b6c-95ce-b90ddb49fced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action [3, 3] - Done: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 1,  1,  1,  1],\n",
       "        [ 1, -1,  1,  1],\n",
       "        [ 1,  1, -1,  1],\n",
       "        [ 1,  1, -1, -1]], dtype=int8),\n",
       " 8.0,\n",
       " True,\n",
       " {})"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.envs[0].env.step(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "edd47c88-4335-4092-a86a-14eac9f83184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1, -1, -1],\n",
       "       [-1,  1, -1, -1],\n",
       "       [-1, -1,  1, -1],\n",
       "       [-1, -1,  1,  1]], dtype=int8)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.envs[0].env.board"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c95848-d644-435e-af95-cf754690dfa1",
   "metadata": {},
   "source": [
    "### Guardar el SelfPlayEnv en el módulo multi_env para poder despues importarla desde otra notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685c447b-def1-46e6-8bf3-299e7e596d2c",
   "metadata": {},
   "source": [
    "# Instanciamos el modelo con MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "16ebead4-efc5-498e-a411-e8f1ef955ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "04ecdb1b-7588-488b-b4ed-5722f2fb2555",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "    'MlpPolicy',\n",
    "    env,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "38ac1fd0-947c-4e8d-b42a-5b18ffc3f687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten()\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential()\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "dd324ce9-6a80-437e-aedd-b44585c99afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "c274276c-25eb-4b94-9841-89e00795fe49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1, 8, 8)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "6736603d-2624-4e97-8d32-dcb251ff1586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([59, 16,  7, 27,  6, 38, 22, 25, 41, 22]), None)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1497081e-e277-4dff-b576-ae58f96ac85b",
   "metadata": {},
   "source": [
    "Observaciones:\n",
    "- Lo primero que hace stablebaselines si ponemos MLP es un flatten\n",
    "- Las acciones predichas por el modelo (sin entrentar) tienen una alta probabildad de ser inválidas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
