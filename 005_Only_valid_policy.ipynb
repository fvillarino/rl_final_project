{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ccb5793f-80b2-4c6b-b15b-51abf33c83e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "50a9f67b-265e-401e-85c7-55b3827c9e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_env import make_reversi_vec_env, SelfPlayEnv\n",
    "import torch as th\n",
    "from players import RandomPlayer\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b07411d1-263f-445f-9dc8-92ef55cd4c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "board_shape = 8\n",
    "n_envs = 1\n",
    "env = make_reversi_vec_env(\n",
    "    SelfPlayEnv, n_envs=n_envs,\n",
    "    env_kwargs={\n",
    "        'board_shape': board_shape,\n",
    "        'LocalPlayer': RandomPlayer\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31f7b6a-ab0b-495f-9b9b-03220308f75b",
   "metadata": {},
   "source": [
    "# Modificación de librería para que haga argmax solo sobre las válidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "65966e76-d302-40f5-be6a-1b00565194cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "    ActorCriticPolicy,\n",
    "    env,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ac109405-906a-4c2e-92bd-0ab6d8146190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([22]), None)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(env.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe8ae4f-2113-4031-8b6a-3b8210285937",
   "metadata": {},
   "source": [
    "# Custom ActorCriticPolicy \n",
    "\n",
    "https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/policies.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d4552377-3076-44dd-a6d4-d504b5915e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from boardgame2 import ReversiEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "13b9569a-4fed-4508-8cbd-73b7aac5058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_not_vect = ReversiEnv(board_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e89c4a9b-1735-4c70-9687-c7a66275cb02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "state = env.reset()[0][0]\n",
    "player = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "58553f62-ab53-41b9-9815-df9706caffc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0]], dtype=int8)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_not_vect.get_valid((state, player))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4898c124-9b43-4088-a366-03adc8b31ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actions_mask(state):\n",
    "    player = 1\n",
    "    valid_actions = env_not_vect.get_valid((state, player))\n",
    "    #print(state)\n",
    "    return valid_actions.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1d92fe71-689f-4a7f-8f0b-4ee7453a4db1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 64 into shape (4,4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-5bf633c9a2e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_actions_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 64 into shape (4,4)"
     ]
    }
   ],
   "source": [
    "get_actions_mask(env.reset()[0][0]).reshape(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a8599b62-07b5-4972-88fc-126faedeb133",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomActorCriticPolicy(ActorCriticPolicy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        #observation_space: gym.spaces.Space,\n",
    "        #action_space: gym.spaces.Space,\n",
    "        #lr_schedule,\n",
    "        #net_arch=None,\n",
    "        #activation_fn=nn.Tanh,\n",
    "        #features_extractor_kwargs=dict(feature_dim=128),\n",
    "        #feature_extractor_class=NewCustomCNN,\n",
    "        *args, # Todos los argumentos posicionales de ActorCriticPolicy\n",
    "        actions_mask_func=None, # El nuevo argumento\n",
    "        **kwargs # Todos los argumentos opcionales de ActorCriticPolicy\n",
    "    ):\n",
    "        super(CustomActorCriticPolicy, self).__init__(\n",
    "            *args,\n",
    "            **kwargs\n",
    "        )\n",
    "        if actions_mask_func:\n",
    "            self.get_actions_mask = actions_mask_func\n",
    "    \n",
    "    \n",
    "    \n",
    "    def sample_masked_actions(self, obs, distribution, deterministic=False, return_distribution=False):\n",
    "        # Dada las obs y distribuciones luego de evaluar la red neuronal, samplear solo las acciones válidas\n",
    "        # Las obs se usan para que con self.get_actions_mask se obtengan las acciones válidas\n",
    "        # las distribuciones son el resultado de evaluar la red neuronal y van a dar acciones no validas\n",
    "        # Generar una nueva distribución (del lado de los logits preferentemente) donde las acciones no válidas\n",
    "        # tengan probabildad nula de ser muestreadas\n",
    "        # Luego se modifican abajo los métodos\n",
    "        # _predict, forward y evaluate_actions\n",
    "        # Si tiene el flag de return_distribution en true devuelve la distribución nueva\n",
    "        # Caso contrario devuelve las acciones\n",
    "        # Para tener en cuenta, obs tiene dimensión [batch_size, channels, H, W]\n",
    "        # Recomendamos poner un print(obs.shape)\n",
    "        # y correr:\n",
    "        # obs = env.reset()\n",
    "        # actions, _ = model.predict(obs)\n",
    "        # Para sacarse las dudas\n",
    "        \n",
    "        def get_mask(obs):\n",
    "            masks = np.zeros((len(obs), obs.shape[-1] * obs.shape[-2]))\n",
    "            for i, board in enumerate(obs):\n",
    "                board = board[0].cpu().numpy()\n",
    "                masks[i] = 1 - self.get_actions_mask(board)\n",
    "            return th.from_numpy(masks).to(self.device)\n",
    "        #print(f'Obs: {obs}')\n",
    "        #print(f'Mask: {masks}')\n",
    "        masks = get_mask(obs)\n",
    "        masks[masks == 1] = -1e6#-np.inf\n",
    "        masked_logits = distribution.logits + masks\n",
    "        \n",
    "        if return_distribution:\n",
    "            return th.distributions.Categorical(logits=masked_logits)\n",
    "        if deterministic:\n",
    "            return th.argmax(masked_logits, axis=1)\n",
    "        return th.distributions.Categorical(logits=masked_logits).sample()\n",
    "    \n",
    "    def _predict(self, observation, deterministic=False):\n",
    "        \"\"\"\n",
    "        Get the action according to the policy for a given observation.\n",
    "        :param observation:\n",
    "        :param deterministic: Whether to use stochastic or deterministic actions\n",
    "        :return: Taken action according to the policy\n",
    "        \"\"\"\n",
    "        latent_pi, _, latent_sde = self._get_latent(observation)\n",
    "        distribution = self._get_action_dist_from_latent(latent_pi, latent_sde)\n",
    "        \n",
    "        #print(f'predict - latent_pi: {latent_pi}')\n",
    "        #print(f'predict - latent_sde: {latent_sde}')\n",
    "        #print(f'predict - distribution: {distribution.get_actions()}')\n",
    "        \n",
    "        if self.get_actions_mask:\n",
    "            #print(f'Entró por get_actions_mask')\n",
    "            actions = self.sample_masked_actions(observation, distribution.distribution, deterministic=deterministic)\n",
    "        else:\n",
    "            actions = distribution.get_actions(deterministic=deterministic)\n",
    "        \n",
    "        #print(f'predict - actions: {actions}')\n",
    "        return actions\n",
    "    \n",
    "    def forward(self, obs: th.Tensor, deterministic: bool = False):\n",
    "        \"\"\"\n",
    "        Forward pass in all the networks (actor and critic)\n",
    "        :param obs: Observation\n",
    "        :param deterministic: Whether to sample or use deterministic actions\n",
    "        :return: action, value and log probability of the action\n",
    "        \"\"\"\n",
    "        latent_pi, latent_vf, latent_sde = self._get_latent(obs)\n",
    "        # Evaluate the values for the given observations\n",
    "        values = self.value_net(latent_vf)\n",
    "        distribution = self._get_action_dist_from_latent(latent_pi, latent_sde=latent_sde)\n",
    "        distrib = self.sample_masked_actions(obs, distribution.distribution, return_distribution=True)\n",
    "\n",
    "        actions = self.sample_masked_actions(obs, distribution.distribution, deterministic=deterministic)\n",
    "        \n",
    "        #if self.get_actions_mask:\n",
    "        #    actions = self.sample_masked_actions(obs, distribution.distribution, deterministic=deterministic)\n",
    "        #else:\n",
    "        #    actions = distribution.get_actions(deterministic=deterministic)\n",
    "\n",
    "        log_prob = distribution.log_prob(actions)\n",
    "        return actions, values, log_prob\n",
    "    \n",
    "    def evaluate_actions(self, obs: th.Tensor, actions: th.Tensor) :#-> tuple[th.Tensor, th.Tensor, th.Tensor]:\n",
    "        \"\"\"\n",
    "        Evaluate actions according to the current policy,\n",
    "        given the observations.\n",
    "        :param obs:\n",
    "        :param actions:\n",
    "        :return: estimated value, log likelihood of taking those actions\n",
    "            and entropy of the action distribution.\n",
    "        \"\"\"\n",
    "        latent_pi, latent_vf, latent_sde = self._get_latent(obs)\n",
    "        distribution = self._get_action_dist_from_latent(latent_pi, latent_sde)\n",
    "        distrib = self.sample_masked_actions(obs, distribution.distribution, return_distribution=True)\n",
    "\n",
    "        log_prob = distrib.log_prob(actions)\n",
    "        values = self.value_net(latent_vf)\n",
    "        return values, log_prob, distrib.entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "994ba701-c17b-43fb-99df-836f2494da54",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "actions, _ = model.predict(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "83565adb-fde5-426a-8c16-eea023e4af3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([41])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4999562e-c666-4501-aa42-9a320c3c201e",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(activation_fn=th.nn.ReLU,\n",
    "                     net_arch=[dict(pi=[32, 32], vf=[32, 32])],\n",
    "                    actions_mask_func=get_actions_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "122d6fef-5538-48d5-bffb-e8f92ab55b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "    CustomActorCriticPolicy,\n",
    "    env,\n",
    "    verbose=0,\n",
    "    policy_kwargs = {'actions_mask_func': get_actions_mask}\n",
    "    #policy_kwargs = policy_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d500b8c8-ffbf-4175-a803-269502c9e044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "      dtype=int8)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testeo de predict\n",
    "model.policy.get_actions_mask(env.reset()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f0a615da-5b15-40d7-b24d-d1b323068574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  1., -1.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., -1.,  1.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "860228c6-91e8-4a2e-925f-993cc700e972",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "actions, _ = model.predict(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8bf9fbd-5077-4255-8eac-e84462c35378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificar que las acciones son válidas\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09a65995-f994-4f0b-a369-4db7c6981a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([20]),\n",
       " tensor([[-0.1257]], grad_fn=<AddmmBackward>),\n",
       " tensor([-4.1631], grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testeo de forward\n",
    "model.policy(th.from_numpy(obs).to(model.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b41a057-bfa4-4901-b102-c3d385dedabf",
   "metadata": {},
   "source": [
    "# Corremos PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bd2b2d58-5bd1-4963-8fc0-3a7c65590b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "board_shape = 8\n",
    "n_envs = 5\n",
    "gamma = 0.99\n",
    "ent_coef = 0.0\n",
    "gae_lambda = 0.95\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b565b606-2942-48a4-af53-5c78608e7707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reversi_PPO_8by8_0.99_0.95_0.0_10_5_masked_actions_25_08\n",
      "./models/Reversi_PPO_8by8_0.99_0.95_0.0_10_5_masked_actions_25_08\n"
     ]
    }
   ],
   "source": [
    "prefix = 'Reversi_PPO'\n",
    "suffix = 'masked_actions_25_08'\n",
    "model_name = f'{prefix}_{board_shape}by{board_shape}_{gamma}_{gae_lambda}_{ent_coef}_{n_epochs}_{n_envs}_{suffix}'\n",
    "best_model_save_path = f'./models/{model_name}'\n",
    "print(model_name)\n",
    "print(best_model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8640461a-dd62-4ba1-a8bd-d03509a2789d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "    CustomActorCriticPolicy, \n",
    "    env,\n",
    "    verbose=0,\n",
    "    tensorboard_log='tensorboard_log',\n",
    "    gamma=gamma,\n",
    "    gae_lambda=gae_lambda,\n",
    "    ent_coef=ent_coef,\n",
    "    n_epochs=n_epochs,\n",
    "    policy_kwargs = {'actions_mask_func': get_actions_mask}\n",
    "    #policy_kwargs = policy_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0c554a4a-2b95-45cf-a96c-9c836ca45232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "147117ff-2ff0-4aa8-a7b6-fb1914d54e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El entorno de evaluación no corre en paralelo por eso uno solo\n",
    "eval_env = make_reversi_vec_env(\n",
    "    SelfPlayEnv, n_envs=1,\n",
    "    env_kwargs={\n",
    "        'board_shape': board_shape,\n",
    "        'LocalPlayer': RandomPlayer\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "80a6e75e-66cd-42d2-a023-ab9f662130b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(\n",
    "    eval_env = eval_env,\n",
    "    eval_freq=1_000,\n",
    "    n_eval_episodes=500,\n",
    "    deterministic=True,\n",
    "    verbose=1,\n",
    "    best_model_save_path=best_model_save_path,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7e5041b9-700a-473a-9102-6becaad5a7d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=0.07 +/- 0.97\n",
      "Episode length: 30.04 +/- 0.60\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=0.10 +/- 0.98\n",
      "Episode length: 30.06 +/- 0.55\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-0.15 +/- 0.96\n",
      "Episode length: 29.91 +/- 0.59\n",
      "Eval num_timesteps=4000, episode_reward=-0.06 +/- 0.98\n",
      "Episode length: 29.96 +/- 0.60\n",
      "Eval num_timesteps=5000, episode_reward=-0.01 +/- 0.98\n",
      "Episode length: 29.85 +/- 1.68\n",
      "Eval num_timesteps=6000, episode_reward=0.06 +/- 0.98\n",
      "Episode length: 29.99 +/- 0.59\n",
      "Eval num_timesteps=7000, episode_reward=0.10 +/- 0.98\n",
      "Episode length: 29.97 +/- 0.60\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=0.05 +/- 0.98\n",
      "Episode length: 29.99 +/- 0.56\n",
      "Eval num_timesteps=9000, episode_reward=0.14 +/- 0.98\n",
      "Episode length: 30.05 +/- 0.57\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=0.21 +/- 0.96\n",
      "Episode length: 29.99 +/- 1.17\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=0.24 +/- 0.95\n",
      "Episode length: 30.05 +/- 0.54\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=0.27 +/- 0.94\n",
      "Episode length: 30.03 +/- 0.60\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=0.17 +/- 0.96\n",
      "Episode length: 30.00 +/- 0.61\n",
      "Eval num_timesteps=14000, episode_reward=0.11 +/- 0.97\n",
      "Episode length: 30.01 +/- 1.11\n",
      "Eval num_timesteps=15000, episode_reward=0.25 +/- 0.95\n",
      "Episode length: 30.03 +/- 0.59\n",
      "Eval num_timesteps=16000, episode_reward=0.23 +/- 0.95\n",
      "Episode length: 30.04 +/- 0.56\n",
      "Eval num_timesteps=17000, episode_reward=0.22 +/- 0.96\n",
      "Episode length: 29.95 +/- 1.26\n",
      "Eval num_timesteps=18000, episode_reward=0.30 +/- 0.93\n",
      "Episode length: 29.96 +/- 1.23\n",
      "New best mean reward!\n",
      "Eval num_timesteps=19000, episode_reward=0.33 +/- 0.93\n",
      "Episode length: 30.04 +/- 0.57\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=0.36 +/- 0.91\n",
      "Episode length: 30.05 +/- 0.58\n",
      "New best mean reward!\n",
      "Eval num_timesteps=21000, episode_reward=0.38 +/- 0.90\n",
      "Episode length: 30.11 +/- 0.57\n",
      "New best mean reward!\n",
      "Eval num_timesteps=22000, episode_reward=0.37 +/- 0.90\n",
      "Episode length: 30.05 +/- 0.62\n",
      "Eval num_timesteps=23000, episode_reward=0.41 +/- 0.89\n",
      "Episode length: 30.03 +/- 1.18\n",
      "New best mean reward!\n",
      "Eval num_timesteps=24000, episode_reward=0.31 +/- 0.93\n",
      "Episode length: 30.01 +/- 1.23\n",
      "Eval num_timesteps=25000, episode_reward=0.33 +/- 0.93\n",
      "Episode length: 30.01 +/- 0.57\n",
      "Eval num_timesteps=26000, episode_reward=0.38 +/- 0.91\n",
      "Episode length: 30.03 +/- 0.60\n",
      "Eval num_timesteps=27000, episode_reward=0.28 +/- 0.94\n",
      "Episode length: 29.99 +/- 0.57\n",
      "Eval num_timesteps=28000, episode_reward=0.30 +/- 0.93\n",
      "Episode length: 30.02 +/- 0.54\n",
      "Eval num_timesteps=29000, episode_reward=0.35 +/- 0.92\n",
      "Episode length: 29.99 +/- 0.88\n",
      "Eval num_timesteps=30000, episode_reward=0.29 +/- 0.94\n",
      "Episode length: 30.03 +/- 0.56\n",
      "Eval num_timesteps=31000, episode_reward=0.31 +/- 0.92\n",
      "Episode length: 29.98 +/- 1.25\n",
      "Eval num_timesteps=32000, episode_reward=0.38 +/- 0.90\n",
      "Episode length: 30.08 +/- 0.55\n",
      "Eval num_timesteps=33000, episode_reward=0.27 +/- 0.95\n",
      "Episode length: 29.96 +/- 1.26\n",
      "Eval num_timesteps=34000, episode_reward=0.39 +/- 0.90\n",
      "Episode length: 30.00 +/- 1.09\n",
      "Eval num_timesteps=35000, episode_reward=0.35 +/- 0.92\n",
      "Episode length: 30.03 +/- 0.55\n",
      "Eval num_timesteps=36000, episode_reward=0.33 +/- 0.92\n",
      "Episode length: 30.05 +/- 0.54\n",
      "Eval num_timesteps=37000, episode_reward=0.32 +/- 0.93\n",
      "Episode length: 30.03 +/- 0.57\n",
      "Eval num_timesteps=38000, episode_reward=0.41 +/- 0.89\n",
      "Episode length: 30.04 +/- 0.59\n",
      "Eval num_timesteps=39000, episode_reward=0.30 +/- 0.93\n",
      "Episode length: 30.00 +/- 1.13\n",
      "Eval num_timesteps=40000, episode_reward=0.33 +/- 0.92\n",
      "Episode length: 30.01 +/- 0.58\n",
      "Eval num_timesteps=41000, episode_reward=0.36 +/- 0.92\n",
      "Episode length: 30.07 +/- 0.54\n",
      "Eval num_timesteps=42000, episode_reward=0.38 +/- 0.91\n",
      "Episode length: 30.00 +/- 1.17\n",
      "Eval num_timesteps=43000, episode_reward=0.41 +/- 0.89\n",
      "Episode length: 30.04 +/- 0.54\n",
      "Eval num_timesteps=44000, episode_reward=0.45 +/- 0.87\n",
      "Episode length: 30.05 +/- 0.55\n",
      "New best mean reward!\n",
      "Eval num_timesteps=45000, episode_reward=0.42 +/- 0.88\n",
      "Episode length: 30.03 +/- 0.52\n",
      "Eval num_timesteps=46000, episode_reward=0.43 +/- 0.88\n",
      "Episode length: 30.04 +/- 0.56\n",
      "Eval num_timesteps=47000, episode_reward=0.42 +/- 0.89\n",
      "Episode length: 30.01 +/- 0.58\n",
      "Eval num_timesteps=48000, episode_reward=0.48 +/- 0.87\n",
      "Episode length: 30.02 +/- 0.57\n",
      "New best mean reward!\n",
      "Eval num_timesteps=49000, episode_reward=0.48 +/- 0.85\n",
      "Episode length: 30.06 +/- 0.53\n",
      "Eval num_timesteps=50000, episode_reward=0.49 +/- 0.85\n",
      "Episode length: 30.02 +/- 0.54\n",
      "New best mean reward!\n",
      "Eval num_timesteps=51000, episode_reward=0.46 +/- 0.87\n",
      "Episode length: 29.95 +/- 1.59\n",
      "Eval num_timesteps=52000, episode_reward=0.53 +/- 0.83\n",
      "Episode length: 30.04 +/- 0.60\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=0.45 +/- 0.88\n",
      "Episode length: 30.02 +/- 0.55\n",
      "Eval num_timesteps=54000, episode_reward=0.49 +/- 0.85\n",
      "Episode length: 30.03 +/- 0.55\n",
      "Eval num_timesteps=55000, episode_reward=0.49 +/- 0.85\n",
      "Episode length: 30.00 +/- 1.04\n",
      "Eval num_timesteps=56000, episode_reward=0.50 +/- 0.84\n",
      "Episode length: 29.97 +/- 1.12\n",
      "Eval num_timesteps=57000, episode_reward=0.48 +/- 0.85\n",
      "Episode length: 29.99 +/- 1.05\n",
      "Eval num_timesteps=58000, episode_reward=0.58 +/- 0.80\n",
      "Episode length: 30.09 +/- 0.56\n",
      "New best mean reward!\n",
      "Eval num_timesteps=59000, episode_reward=0.58 +/- 0.80\n",
      "Episode length: 30.06 +/- 0.56\n",
      "Eval num_timesteps=60000, episode_reward=0.50 +/- 0.85\n",
      "Episode length: 30.04 +/- 0.56\n",
      "Eval num_timesteps=61000, episode_reward=0.52 +/- 0.84\n",
      "Episode length: 30.06 +/- 0.56\n",
      "Eval num_timesteps=62000, episode_reward=0.50 +/- 0.85\n",
      "Episode length: 30.01 +/- 1.22\n",
      "Eval num_timesteps=63000, episode_reward=0.49 +/- 0.85\n",
      "Episode length: 30.09 +/- 0.58\n",
      "Eval num_timesteps=64000, episode_reward=0.52 +/- 0.84\n",
      "Episode length: 30.04 +/- 0.54\n",
      "Eval num_timesteps=65000, episode_reward=0.59 +/- 0.79\n",
      "Episode length: 30.11 +/- 0.58\n",
      "New best mean reward!\n",
      "Eval num_timesteps=66000, episode_reward=0.58 +/- 0.80\n",
      "Episode length: 30.02 +/- 1.45\n",
      "Eval num_timesteps=67000, episode_reward=0.49 +/- 0.85\n",
      "Episode length: 30.08 +/- 0.58\n",
      "Eval num_timesteps=68000, episode_reward=0.50 +/- 0.84\n",
      "Episode length: 30.05 +/- 0.55\n",
      "Eval num_timesteps=69000, episode_reward=0.54 +/- 0.82\n",
      "Episode length: 30.06 +/- 0.53\n",
      "Eval num_timesteps=70000, episode_reward=0.51 +/- 0.84\n",
      "Episode length: 30.04 +/- 0.53\n",
      "Eval num_timesteps=71000, episode_reward=0.57 +/- 0.80\n",
      "Episode length: 30.02 +/- 1.09\n",
      "Eval num_timesteps=72000, episode_reward=0.56 +/- 0.81\n",
      "Episode length: 30.04 +/- 0.53\n",
      "Eval num_timesteps=73000, episode_reward=0.54 +/- 0.82\n",
      "Episode length: 30.07 +/- 0.56\n",
      "Eval num_timesteps=74000, episode_reward=0.57 +/- 0.80\n",
      "Episode length: 30.09 +/- 0.56\n",
      "Eval num_timesteps=75000, episode_reward=0.52 +/- 0.84\n",
      "Episode length: 30.05 +/- 0.54\n",
      "Eval num_timesteps=76000, episode_reward=0.56 +/- 0.82\n",
      "Episode length: 30.08 +/- 0.54\n",
      "Eval num_timesteps=77000, episode_reward=0.47 +/- 0.86\n",
      "Episode length: 30.07 +/- 0.60\n",
      "Eval num_timesteps=78000, episode_reward=0.50 +/- 0.86\n",
      "Episode length: 29.99 +/- 1.22\n",
      "Eval num_timesteps=79000, episode_reward=0.59 +/- 0.80\n",
      "Episode length: 30.06 +/- 1.06\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=0.51 +/- 0.83\n",
      "Episode length: 30.03 +/- 1.11\n",
      "Eval num_timesteps=81000, episode_reward=0.52 +/- 0.83\n",
      "Episode length: 30.08 +/- 0.56\n",
      "Eval num_timesteps=82000, episode_reward=0.54 +/- 0.82\n",
      "Episode length: 30.04 +/- 1.14\n",
      "Eval num_timesteps=83000, episode_reward=0.54 +/- 0.82\n",
      "Episode length: 30.06 +/- 0.61\n",
      "Eval num_timesteps=84000, episode_reward=0.59 +/- 0.79\n",
      "Episode length: 30.05 +/- 0.57\n",
      "Eval num_timesteps=85000, episode_reward=0.55 +/- 0.82\n",
      "Episode length: 30.02 +/- 0.56\n",
      "Eval num_timesteps=86000, episode_reward=0.57 +/- 0.80\n",
      "Episode length: 30.00 +/- 1.27\n",
      "Eval num_timesteps=87000, episode_reward=0.62 +/- 0.78\n",
      "Episode length: 30.11 +/- 0.58\n",
      "New best mean reward!\n",
      "Eval num_timesteps=88000, episode_reward=0.58 +/- 0.79\n",
      "Episode length: 30.05 +/- 0.55\n",
      "Eval num_timesteps=89000, episode_reward=0.56 +/- 0.81\n",
      "Episode length: 30.05 +/- 0.60\n",
      "Eval num_timesteps=90000, episode_reward=0.64 +/- 0.76\n",
      "Episode length: 30.10 +/- 0.56\n",
      "New best mean reward!\n",
      "Eval num_timesteps=91000, episode_reward=0.58 +/- 0.79\n",
      "Episode length: 30.09 +/- 0.60\n",
      "Eval num_timesteps=92000, episode_reward=0.60 +/- 0.78\n",
      "Episode length: 30.05 +/- 0.57\n",
      "Eval num_timesteps=93000, episode_reward=0.54 +/- 0.82\n",
      "Episode length: 30.03 +/- 0.58\n",
      "Eval num_timesteps=94000, episode_reward=0.63 +/- 0.76\n",
      "Episode length: 30.06 +/- 1.22\n",
      "Eval num_timesteps=95000, episode_reward=0.61 +/- 0.77\n",
      "Episode length: 30.13 +/- 0.57\n",
      "Eval num_timesteps=96000, episode_reward=0.58 +/- 0.79\n",
      "Episode length: 30.04 +/- 1.15\n",
      "Eval num_timesteps=97000, episode_reward=0.64 +/- 0.76\n",
      "Episode length: 30.08 +/- 0.56\n",
      "Eval num_timesteps=98000, episode_reward=0.61 +/- 0.78\n",
      "Episode length: 30.05 +/- 0.57\n",
      "Eval num_timesteps=99000, episode_reward=0.61 +/- 0.78\n",
      "Episode length: 30.10 +/- 0.57\n",
      "Eval num_timesteps=100000, episode_reward=0.56 +/- 0.81\n",
      "Episode length: 30.05 +/- 0.74\n",
      "Eval num_timesteps=101000, episode_reward=0.56 +/- 0.82\n",
      "Episode length: 30.02 +/- 1.15\n",
      "Eval num_timesteps=102000, episode_reward=0.59 +/- 0.79\n",
      "Episode length: 30.01 +/- 1.21\n",
      "Eval num_timesteps=103000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.07 +/- 0.56\n",
      "Eval num_timesteps=104000, episode_reward=0.57 +/- 0.80\n",
      "Episode length: 30.05 +/- 0.55\n",
      "Eval num_timesteps=105000, episode_reward=0.63 +/- 0.76\n",
      "Episode length: 30.03 +/- 0.59\n",
      "Eval num_timesteps=106000, episode_reward=0.60 +/- 0.78\n",
      "Episode length: 30.06 +/- 0.61\n",
      "Eval num_timesteps=107000, episode_reward=0.60 +/- 0.77\n",
      "Episode length: 30.06 +/- 0.55\n",
      "Eval num_timesteps=108000, episode_reward=0.56 +/- 0.81\n",
      "Episode length: 30.01 +/- 0.57\n",
      "Eval num_timesteps=109000, episode_reward=0.61 +/- 0.77\n",
      "Episode length: 30.05 +/- 0.56\n",
      "Eval num_timesteps=110000, episode_reward=0.65 +/- 0.75\n",
      "Episode length: 30.02 +/- 1.19\n",
      "New best mean reward!\n",
      "Eval num_timesteps=111000, episode_reward=0.60 +/- 0.78\n",
      "Episode length: 30.04 +/- 0.57\n",
      "Eval num_timesteps=112000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.03 +/- 0.55\n",
      "New best mean reward!\n",
      "Eval num_timesteps=113000, episode_reward=0.62 +/- 0.76\n",
      "Episode length: 30.01 +/- 1.20\n",
      "Eval num_timesteps=114000, episode_reward=0.57 +/- 0.81\n",
      "Episode length: 30.01 +/- 0.52\n",
      "Eval num_timesteps=115000, episode_reward=0.66 +/- 0.73\n",
      "Episode length: 30.09 +/- 0.57\n",
      "New best mean reward!\n",
      "Eval num_timesteps=116000, episode_reward=0.60 +/- 0.79\n",
      "Episode length: 29.99 +/- 0.56\n",
      "Eval num_timesteps=117000, episode_reward=0.64 +/- 0.75\n",
      "Episode length: 29.99 +/- 1.11\n",
      "Eval num_timesteps=118000, episode_reward=0.67 +/- 0.72\n",
      "Episode length: 29.97 +/- 0.57\n",
      "New best mean reward!\n",
      "Eval num_timesteps=119000, episode_reward=0.59 +/- 0.79\n",
      "Episode length: 29.98 +/- 1.47\n",
      "Eval num_timesteps=120000, episode_reward=0.68 +/- 0.72\n",
      "Episode length: 30.03 +/- 0.55\n",
      "New best mean reward!\n",
      "Eval num_timesteps=121000, episode_reward=0.63 +/- 0.76\n",
      "Episode length: 29.98 +/- 1.16\n",
      "Eval num_timesteps=122000, episode_reward=0.69 +/- 0.69\n",
      "Episode length: 30.07 +/- 0.58\n",
      "New best mean reward!\n",
      "Eval num_timesteps=123000, episode_reward=0.55 +/- 0.81\n",
      "Episode length: 30.00 +/- 0.58\n",
      "Eval num_timesteps=124000, episode_reward=0.63 +/- 0.76\n",
      "Episode length: 30.01 +/- 1.17\n",
      "Eval num_timesteps=125000, episode_reward=0.61 +/- 0.78\n",
      "Episode length: 30.05 +/- 1.06\n",
      "Eval num_timesteps=126000, episode_reward=0.70 +/- 0.69\n",
      "Episode length: 30.11 +/- 0.60\n",
      "New best mean reward!\n",
      "Eval num_timesteps=127000, episode_reward=0.73 +/- 0.67\n",
      "Episode length: 30.11 +/- 1.13\n",
      "New best mean reward!\n",
      "Eval num_timesteps=128000, episode_reward=0.72 +/- 0.67\n",
      "Episode length: 30.07 +/- 0.55\n",
      "Eval num_timesteps=129000, episode_reward=0.66 +/- 0.73\n",
      "Episode length: 30.06 +/- 0.57\n",
      "Eval num_timesteps=130000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.00 +/- 1.16\n",
      "Eval num_timesteps=131000, episode_reward=0.66 +/- 0.72\n",
      "Episode length: 30.03 +/- 1.06\n",
      "Eval num_timesteps=132000, episode_reward=0.64 +/- 0.76\n",
      "Episode length: 30.03 +/- 1.11\n",
      "Eval num_timesteps=133000, episode_reward=0.68 +/- 0.70\n",
      "Episode length: 30.07 +/- 1.15\n",
      "Eval num_timesteps=134000, episode_reward=0.62 +/- 0.76\n",
      "Episode length: 30.07 +/- 1.01\n",
      "Eval num_timesteps=135000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.06 +/- 0.54\n",
      "Eval num_timesteps=136000, episode_reward=0.61 +/- 0.77\n",
      "Episode length: 30.06 +/- 0.58\n",
      "Eval num_timesteps=137000, episode_reward=0.64 +/- 0.76\n",
      "Episode length: 30.05 +/- 0.58\n",
      "Eval num_timesteps=138000, episode_reward=0.64 +/- 0.75\n",
      "Episode length: 30.09 +/- 0.59\n",
      "Eval num_timesteps=139000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.10 +/- 0.58\n",
      "Eval num_timesteps=140000, episode_reward=0.62 +/- 0.76\n",
      "Episode length: 30.12 +/- 0.60\n",
      "Eval num_timesteps=141000, episode_reward=0.70 +/- 0.70\n",
      "Episode length: 30.08 +/- 0.57\n",
      "Eval num_timesteps=142000, episode_reward=0.70 +/- 0.70\n",
      "Episode length: 30.06 +/- 1.08\n",
      "Eval num_timesteps=143000, episode_reward=0.70 +/- 0.69\n",
      "Episode length: 30.12 +/- 0.56\n",
      "Eval num_timesteps=144000, episode_reward=0.68 +/- 0.72\n",
      "Episode length: 30.03 +/- 0.56\n",
      "Eval num_timesteps=145000, episode_reward=0.71 +/- 0.69\n",
      "Episode length: 30.14 +/- 0.54\n",
      "Eval num_timesteps=146000, episode_reward=0.67 +/- 0.72\n",
      "Episode length: 30.05 +/- 1.19\n",
      "Eval num_timesteps=147000, episode_reward=0.66 +/- 0.74\n",
      "Episode length: 30.02 +/- 1.45\n",
      "Eval num_timesteps=148000, episode_reward=0.69 +/- 0.70\n",
      "Episode length: 30.11 +/- 0.55\n",
      "Eval num_timesteps=149000, episode_reward=0.68 +/- 0.72\n",
      "Episode length: 30.09 +/- 0.54\n",
      "Eval num_timesteps=150000, episode_reward=0.67 +/- 0.72\n",
      "Episode length: 30.05 +/- 1.11\n",
      "Eval num_timesteps=151000, episode_reward=0.63 +/- 0.76\n",
      "Episode length: 30.14 +/- 0.58\n",
      "Eval num_timesteps=152000, episode_reward=0.67 +/- 0.72\n",
      "Episode length: 30.12 +/- 0.59\n",
      "Eval num_timesteps=153000, episode_reward=0.69 +/- 0.71\n",
      "Episode length: 30.05 +/- 1.32\n",
      "Eval num_timesteps=154000, episode_reward=0.70 +/- 0.69\n",
      "Episode length: 30.11 +/- 1.18\n",
      "Eval num_timesteps=155000, episode_reward=0.70 +/- 0.69\n",
      "Episode length: 30.11 +/- 0.62\n",
      "Eval num_timesteps=156000, episode_reward=0.59 +/- 0.79\n",
      "Episode length: 30.11 +/- 0.60\n",
      "Eval num_timesteps=157000, episode_reward=0.67 +/- 0.72\n",
      "Episode length: 30.13 +/- 0.61\n",
      "Eval num_timesteps=158000, episode_reward=0.68 +/- 0.72\n",
      "Episode length: 30.11 +/- 0.54\n",
      "Eval num_timesteps=159000, episode_reward=0.63 +/- 0.76\n",
      "Episode length: 30.05 +/- 1.18\n",
      "Eval num_timesteps=160000, episode_reward=0.68 +/- 0.71\n",
      "Episode length: 30.11 +/- 0.56\n",
      "Eval num_timesteps=161000, episode_reward=0.70 +/- 0.70\n",
      "Episode length: 30.14 +/- 0.58\n",
      "Eval num_timesteps=162000, episode_reward=0.65 +/- 0.75\n",
      "Episode length: 30.09 +/- 0.59\n",
      "Eval num_timesteps=163000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.13 +/- 0.59\n",
      "Eval num_timesteps=164000, episode_reward=0.68 +/- 0.71\n",
      "Episode length: 30.11 +/- 0.55\n",
      "Eval num_timesteps=165000, episode_reward=0.64 +/- 0.74\n",
      "Episode length: 30.12 +/- 0.60\n",
      "Eval num_timesteps=166000, episode_reward=0.65 +/- 0.75\n",
      "Episode length: 30.06 +/- 1.17\n",
      "Eval num_timesteps=167000, episode_reward=0.68 +/- 0.72\n",
      "Episode length: 30.07 +/- 1.21\n",
      "Eval num_timesteps=168000, episode_reward=0.70 +/- 0.69\n",
      "Episode length: 29.87 +/- 2.12\n",
      "Eval num_timesteps=169000, episode_reward=0.64 +/- 0.75\n",
      "Episode length: 30.10 +/- 0.58\n",
      "Eval num_timesteps=170000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.09 +/- 0.54\n",
      "Eval num_timesteps=171000, episode_reward=0.61 +/- 0.78\n",
      "Episode length: 30.13 +/- 0.61\n",
      "Eval num_timesteps=172000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.02 +/- 1.19\n",
      "Eval num_timesteps=173000, episode_reward=0.63 +/- 0.76\n",
      "Episode length: 30.00 +/- 1.51\n",
      "Eval num_timesteps=174000, episode_reward=0.61 +/- 0.77\n",
      "Episode length: 30.06 +/- 0.58\n",
      "Eval num_timesteps=175000, episode_reward=0.64 +/- 0.76\n",
      "Episode length: 30.09 +/- 0.55\n",
      "Eval num_timesteps=176000, episode_reward=0.58 +/- 0.80\n",
      "Episode length: 30.07 +/- 1.18\n",
      "Eval num_timesteps=177000, episode_reward=0.59 +/- 0.79\n",
      "Episode length: 30.07 +/- 1.06\n",
      "Eval num_timesteps=178000, episode_reward=0.68 +/- 0.71\n",
      "Episode length: 30.00 +/- 1.51\n",
      "Eval num_timesteps=179000, episode_reward=0.59 +/- 0.79\n",
      "Episode length: 30.05 +/- 0.54\n",
      "Eval num_timesteps=180000, episode_reward=0.61 +/- 0.77\n",
      "Episode length: 30.02 +/- 1.13\n",
      "Eval num_timesteps=181000, episode_reward=0.62 +/- 0.76\n",
      "Episode length: 30.04 +/- 1.16\n",
      "Eval num_timesteps=182000, episode_reward=0.61 +/- 0.78\n",
      "Episode length: 30.04 +/- 1.12\n",
      "Eval num_timesteps=183000, episode_reward=0.63 +/- 0.76\n",
      "Episode length: 30.03 +/- 0.58\n",
      "Eval num_timesteps=184000, episode_reward=0.64 +/- 0.74\n",
      "Episode length: 30.11 +/- 0.55\n",
      "Eval num_timesteps=185000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.05 +/- 0.55\n",
      "Eval num_timesteps=186000, episode_reward=0.64 +/- 0.74\n",
      "Episode length: 29.95 +/- 1.67\n",
      "Eval num_timesteps=187000, episode_reward=0.57 +/- 0.80\n",
      "Episode length: 30.04 +/- 0.59\n",
      "Eval num_timesteps=188000, episode_reward=0.64 +/- 0.75\n",
      "Episode length: 30.09 +/- 0.56\n",
      "Eval num_timesteps=189000, episode_reward=0.58 +/- 0.79\n",
      "Episode length: 29.96 +/- 1.57\n",
      "Eval num_timesteps=190000, episode_reward=0.64 +/- 0.74\n",
      "Episode length: 30.06 +/- 0.60\n",
      "Eval num_timesteps=191000, episode_reward=0.58 +/- 0.80\n",
      "Episode length: 29.80 +/- 2.09\n",
      "Eval num_timesteps=192000, episode_reward=0.52 +/- 0.83\n",
      "Episode length: 30.01 +/- 0.95\n",
      "Eval num_timesteps=193000, episode_reward=0.53 +/- 0.83\n",
      "Episode length: 30.06 +/- 0.51\n",
      "Eval num_timesteps=194000, episode_reward=0.50 +/- 0.85\n",
      "Episode length: 29.99 +/- 0.58\n",
      "Eval num_timesteps=195000, episode_reward=0.61 +/- 0.78\n",
      "Episode length: 29.99 +/- 1.50\n",
      "Eval num_timesteps=196000, episode_reward=0.45 +/- 0.88\n",
      "Episode length: 29.97 +/- 1.20\n",
      "Eval num_timesteps=197000, episode_reward=0.57 +/- 0.80\n",
      "Episode length: 30.04 +/- 0.62\n",
      "Eval num_timesteps=198000, episode_reward=0.54 +/- 0.82\n",
      "Episode length: 29.93 +/- 1.52\n",
      "Eval num_timesteps=199000, episode_reward=0.60 +/- 0.77\n",
      "Episode length: 30.03 +/- 0.98\n",
      "Eval num_timesteps=200000, episode_reward=0.56 +/- 0.81\n",
      "Episode length: 29.99 +/- 1.13\n",
      "Eval num_timesteps=201000, episode_reward=0.62 +/- 0.76\n",
      "Episode length: 30.01 +/- 1.16\n",
      "Eval num_timesteps=202000, episode_reward=0.56 +/- 0.81\n",
      "Episode length: 30.05 +/- 0.54\n",
      "Eval num_timesteps=203000, episode_reward=0.56 +/- 0.80\n",
      "Episode length: 30.02 +/- 0.58\n",
      "Eval num_timesteps=204000, episode_reward=0.60 +/- 0.79\n",
      "Episode length: 30.03 +/- 0.55\n",
      "Eval num_timesteps=205000, episode_reward=0.50 +/- 0.85\n",
      "Episode length: 30.05 +/- 0.60\n",
      "Eval num_timesteps=206000, episode_reward=0.64 +/- 0.75\n",
      "Episode length: 30.03 +/- 0.58\n",
      "Eval num_timesteps=207000, episode_reward=0.57 +/- 0.79\n",
      "Episode length: 30.04 +/- 0.54\n",
      "Eval num_timesteps=208000, episode_reward=0.55 +/- 0.82\n",
      "Episode length: 30.01 +/- 0.56\n",
      "Eval num_timesteps=209000, episode_reward=0.55 +/- 0.81\n",
      "Episode length: 29.92 +/- 1.43\n",
      "Eval num_timesteps=210000, episode_reward=0.54 +/- 0.82\n",
      "Episode length: 29.98 +/- 1.17\n",
      "Eval num_timesteps=211000, episode_reward=0.54 +/- 0.82\n",
      "Episode length: 30.02 +/- 0.55\n",
      "Eval num_timesteps=212000, episode_reward=0.58 +/- 0.79\n",
      "Episode length: 30.00 +/- 1.13\n",
      "Eval num_timesteps=213000, episode_reward=0.60 +/- 0.77\n",
      "Episode length: 30.09 +/- 0.58\n",
      "Eval num_timesteps=214000, episode_reward=0.64 +/- 0.75\n",
      "Episode length: 30.07 +/- 0.56\n",
      "Eval num_timesteps=215000, episode_reward=0.58 +/- 0.80\n",
      "Episode length: 30.04 +/- 0.79\n",
      "Eval num_timesteps=216000, episode_reward=0.52 +/- 0.83\n",
      "Episode length: 29.97 +/- 1.06\n",
      "Eval num_timesteps=217000, episode_reward=0.63 +/- 0.75\n",
      "Episode length: 30.10 +/- 0.58\n",
      "Eval num_timesteps=218000, episode_reward=0.55 +/- 0.81\n",
      "Episode length: 30.08 +/- 0.60\n",
      "Eval num_timesteps=219000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.04 +/- 0.60\n",
      "Eval num_timesteps=220000, episode_reward=0.61 +/- 0.77\n",
      "Episode length: 30.10 +/- 0.59\n",
      "Eval num_timesteps=221000, episode_reward=0.58 +/- 0.80\n",
      "Episode length: 30.03 +/- 0.55\n",
      "Eval num_timesteps=222000, episode_reward=0.59 +/- 0.79\n",
      "Episode length: 30.08 +/- 0.58\n",
      "Eval num_timesteps=223000, episode_reward=0.59 +/- 0.80\n",
      "Episode length: 30.09 +/- 0.57\n",
      "Eval num_timesteps=224000, episode_reward=0.61 +/- 0.77\n",
      "Episode length: 30.04 +/- 1.23\n",
      "Eval num_timesteps=225000, episode_reward=0.55 +/- 0.82\n",
      "Episode length: 30.03 +/- 1.18\n",
      "Eval num_timesteps=226000, episode_reward=0.62 +/- 0.76\n",
      "Episode length: 30.12 +/- 0.57\n",
      "Eval num_timesteps=227000, episode_reward=0.59 +/- 0.79\n",
      "Episode length: 30.09 +/- 0.54\n",
      "Eval num_timesteps=228000, episode_reward=0.61 +/- 0.77\n",
      "Episode length: 30.09 +/- 0.55\n",
      "Eval num_timesteps=229000, episode_reward=0.63 +/- 0.75\n",
      "Episode length: 30.08 +/- 1.00\n",
      "Eval num_timesteps=230000, episode_reward=0.59 +/- 0.80\n",
      "Episode length: 30.10 +/- 0.55\n",
      "Eval num_timesteps=231000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.13 +/- 0.61\n",
      "Eval num_timesteps=232000, episode_reward=0.59 +/- 0.80\n",
      "Episode length: 30.04 +/- 1.11\n",
      "Eval num_timesteps=233000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.09 +/- 0.57\n",
      "Eval num_timesteps=234000, episode_reward=0.56 +/- 0.81\n",
      "Episode length: 30.09 +/- 0.56\n",
      "Eval num_timesteps=235000, episode_reward=0.58 +/- 0.79\n",
      "Episode length: 30.06 +/- 0.57\n",
      "Eval num_timesteps=236000, episode_reward=0.61 +/- 0.77\n",
      "Episode length: 30.12 +/- 0.58\n",
      "Eval num_timesteps=237000, episode_reward=0.63 +/- 0.75\n",
      "Episode length: 30.05 +/- 1.20\n",
      "Eval num_timesteps=238000, episode_reward=0.60 +/- 0.79\n",
      "Episode length: 30.03 +/- 0.58\n",
      "Eval num_timesteps=239000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.03 +/- 0.52\n",
      "Eval num_timesteps=240000, episode_reward=0.68 +/- 0.71\n",
      "Episode length: 30.07 +/- 0.53\n",
      "Eval num_timesteps=241000, episode_reward=0.59 +/- 0.79\n",
      "Episode length: 30.08 +/- 0.55\n",
      "Eval num_timesteps=242000, episode_reward=0.68 +/- 0.72\n",
      "Episode length: 30.10 +/- 0.59\n",
      "Eval num_timesteps=243000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.08 +/- 0.53\n",
      "Eval num_timesteps=244000, episode_reward=0.61 +/- 0.76\n",
      "Episode length: 30.07 +/- 0.57\n",
      "Eval num_timesteps=245000, episode_reward=0.68 +/- 0.71\n",
      "Episode length: 30.10 +/- 0.55\n",
      "Eval num_timesteps=246000, episode_reward=0.59 +/- 0.78\n",
      "Episode length: 30.07 +/- 0.58\n",
      "Eval num_timesteps=247000, episode_reward=0.63 +/- 0.74\n",
      "Episode length: 30.08 +/- 0.53\n",
      "Eval num_timesteps=248000, episode_reward=0.62 +/- 0.76\n",
      "Episode length: 30.08 +/- 1.19\n",
      "Eval num_timesteps=249000, episode_reward=0.60 +/- 0.78\n",
      "Episode length: 30.08 +/- 0.59\n",
      "Eval num_timesteps=250000, episode_reward=0.59 +/- 0.79\n",
      "Episode length: 30.07 +/- 0.89\n",
      "Eval num_timesteps=251000, episode_reward=0.59 +/- 0.78\n",
      "Episode length: 30.10 +/- 1.18\n",
      "Eval num_timesteps=252000, episode_reward=0.59 +/- 0.78\n",
      "Episode length: 30.10 +/- 0.57\n",
      "Eval num_timesteps=253000, episode_reward=0.57 +/- 0.80\n",
      "Episode length: 30.02 +/- 1.02\n",
      "Eval num_timesteps=254000, episode_reward=0.58 +/- 0.79\n",
      "Episode length: 30.06 +/- 1.11\n",
      "Eval num_timesteps=255000, episode_reward=0.63 +/- 0.76\n",
      "Episode length: 30.15 +/- 0.55\n",
      "Eval num_timesteps=256000, episode_reward=0.63 +/- 0.76\n",
      "Episode length: 30.11 +/- 0.55\n",
      "Eval num_timesteps=257000, episode_reward=0.60 +/- 0.78\n",
      "Episode length: 30.13 +/- 0.56\n",
      "Eval num_timesteps=258000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.05 +/- 0.98\n",
      "Eval num_timesteps=259000, episode_reward=0.59 +/- 0.79\n",
      "Episode length: 30.07 +/- 0.56\n",
      "Eval num_timesteps=260000, episode_reward=0.60 +/- 0.78\n",
      "Episode length: 30.11 +/- 0.57\n",
      "Eval num_timesteps=261000, episode_reward=0.61 +/- 0.78\n",
      "Episode length: 30.09 +/- 0.57\n",
      "Eval num_timesteps=262000, episode_reward=0.54 +/- 0.83\n",
      "Episode length: 30.04 +/- 1.04\n",
      "Eval num_timesteps=263000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.10 +/- 0.57\n",
      "Eval num_timesteps=264000, episode_reward=0.58 +/- 0.79\n",
      "Episode length: 30.10 +/- 0.55\n",
      "Eval num_timesteps=265000, episode_reward=0.53 +/- 0.83\n",
      "Episode length: 30.01 +/- 0.91\n",
      "Eval num_timesteps=266000, episode_reward=0.58 +/- 0.80\n",
      "Episode length: 30.06 +/- 0.57\n",
      "Eval num_timesteps=267000, episode_reward=0.56 +/- 0.81\n",
      "Episode length: 30.01 +/- 1.10\n",
      "Eval num_timesteps=268000, episode_reward=0.51 +/- 0.84\n",
      "Episode length: 30.08 +/- 0.59\n",
      "Eval num_timesteps=269000, episode_reward=0.55 +/- 0.82\n",
      "Episode length: 30.08 +/- 0.52\n",
      "Eval num_timesteps=270000, episode_reward=0.58 +/- 0.80\n",
      "Episode length: 30.09 +/- 0.56\n",
      "Eval num_timesteps=271000, episode_reward=0.56 +/- 0.81\n",
      "Episode length: 30.07 +/- 0.57\n",
      "Eval num_timesteps=272000, episode_reward=0.56 +/- 0.81\n",
      "Episode length: 30.05 +/- 0.99\n",
      "Eval num_timesteps=273000, episode_reward=0.50 +/- 0.85\n",
      "Episode length: 30.07 +/- 0.62\n",
      "Eval num_timesteps=274000, episode_reward=0.59 +/- 0.78\n",
      "Episode length: 30.04 +/- 1.14\n",
      "Eval num_timesteps=275000, episode_reward=0.58 +/- 0.80\n",
      "Episode length: 30.11 +/- 0.57\n",
      "Eval num_timesteps=276000, episode_reward=0.54 +/- 0.82\n",
      "Episode length: 30.04 +/- 0.61\n",
      "Eval num_timesteps=277000, episode_reward=0.60 +/- 0.78\n",
      "Episode length: 30.08 +/- 0.56\n",
      "Eval num_timesteps=278000, episode_reward=0.51 +/- 0.84\n",
      "Episode length: 30.04 +/- 0.58\n",
      "Eval num_timesteps=279000, episode_reward=0.55 +/- 0.82\n",
      "Episode length: 30.07 +/- 0.59\n",
      "Eval num_timesteps=280000, episode_reward=0.58 +/- 0.79\n",
      "Episode length: 30.07 +/- 0.59\n",
      "Eval num_timesteps=281000, episode_reward=0.55 +/- 0.82\n",
      "Episode length: 30.04 +/- 0.59\n",
      "Eval num_timesteps=282000, episode_reward=0.59 +/- 0.79\n",
      "Episode length: 30.06 +/- 0.57\n",
      "Eval num_timesteps=283000, episode_reward=0.53 +/- 0.83\n",
      "Episode length: 30.05 +/- 0.64\n",
      "Eval num_timesteps=284000, episode_reward=0.58 +/- 0.80\n",
      "Episode length: 30.08 +/- 0.61\n",
      "Eval num_timesteps=285000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.09 +/- 0.55\n",
      "Eval num_timesteps=286000, episode_reward=0.59 +/- 0.79\n",
      "Episode length: 29.98 +/- 1.59\n",
      "Eval num_timesteps=287000, episode_reward=0.54 +/- 0.82\n",
      "Episode length: 30.08 +/- 0.61\n",
      "Eval num_timesteps=288000, episode_reward=0.61 +/- 0.77\n",
      "Episode length: 30.04 +/- 0.57\n",
      "Eval num_timesteps=289000, episode_reward=0.64 +/- 0.76\n",
      "Episode length: 30.12 +/- 0.58\n",
      "Eval num_timesteps=290000, episode_reward=0.54 +/- 0.83\n",
      "Episode length: 30.08 +/- 0.60\n",
      "Eval num_timesteps=291000, episode_reward=0.51 +/- 0.85\n",
      "Episode length: 30.04 +/- 0.59\n",
      "Eval num_timesteps=292000, episode_reward=0.53 +/- 0.83\n",
      "Episode length: 30.08 +/- 0.60\n",
      "Eval num_timesteps=293000, episode_reward=0.57 +/- 0.80\n",
      "Episode length: 30.04 +/- 0.57\n",
      "Eval num_timesteps=294000, episode_reward=0.59 +/- 0.79\n",
      "Episode length: 30.13 +/- 0.58\n",
      "Eval num_timesteps=295000, episode_reward=0.51 +/- 0.85\n",
      "Episode length: 30.08 +/- 0.56\n",
      "Eval num_timesteps=296000, episode_reward=0.64 +/- 0.75\n",
      "Episode length: 30.07 +/- 0.56\n",
      "Eval num_timesteps=297000, episode_reward=0.59 +/- 0.79\n",
      "Episode length: 30.10 +/- 0.55\n",
      "Eval num_timesteps=298000, episode_reward=0.57 +/- 0.79\n",
      "Episode length: 30.07 +/- 0.52\n",
      "Eval num_timesteps=299000, episode_reward=0.55 +/- 0.82\n",
      "Episode length: 30.08 +/- 0.57\n",
      "Eval num_timesteps=300000, episode_reward=0.55 +/- 0.82\n",
      "Episode length: 30.10 +/- 0.58\n",
      "Eval num_timesteps=301000, episode_reward=0.56 +/- 0.80\n",
      "Episode length: 30.10 +/- 0.54\n",
      "Eval num_timesteps=302000, episode_reward=0.63 +/- 0.75\n",
      "Episode length: 30.08 +/- 0.53\n",
      "Eval num_timesteps=303000, episode_reward=0.61 +/- 0.77\n",
      "Episode length: 30.03 +/- 0.54\n",
      "Eval num_timesteps=304000, episode_reward=0.62 +/- 0.76\n",
      "Episode length: 30.11 +/- 0.58\n",
      "Eval num_timesteps=305000, episode_reward=0.49 +/- 0.85\n",
      "Episode length: 30.09 +/- 1.14\n",
      "Eval num_timesteps=306000, episode_reward=0.51 +/- 0.84\n",
      "Episode length: 30.05 +/- 0.58\n",
      "Eval num_timesteps=307000, episode_reward=0.53 +/- 0.82\n",
      "Episode length: 30.06 +/- 0.57\n",
      "Eval num_timesteps=308000, episode_reward=0.58 +/- 0.80\n",
      "Episode length: 29.91 +/- 1.75\n",
      "Eval num_timesteps=309000, episode_reward=0.58 +/- 0.79\n",
      "Episode length: 30.04 +/- 0.59\n",
      "Eval num_timesteps=310000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.08 +/- 0.57\n",
      "Eval num_timesteps=311000, episode_reward=0.56 +/- 0.81\n",
      "Episode length: 30.03 +/- 1.22\n",
      "Eval num_timesteps=312000, episode_reward=0.55 +/- 0.82\n",
      "Episode length: 30.00 +/- 0.58\n",
      "Eval num_timesteps=313000, episode_reward=0.63 +/- 0.75\n",
      "Episode length: 30.09 +/- 0.56\n",
      "Eval num_timesteps=314000, episode_reward=0.55 +/- 0.82\n",
      "Episode length: 30.02 +/- 0.53\n",
      "Eval num_timesteps=315000, episode_reward=0.56 +/- 0.80\n",
      "Episode length: 30.08 +/- 0.59\n",
      "Eval num_timesteps=316000, episode_reward=0.59 +/- 0.79\n",
      "Episode length: 30.04 +/- 0.57\n",
      "Eval num_timesteps=317000, episode_reward=0.65 +/- 0.75\n",
      "Episode length: 30.02 +/- 1.23\n",
      "Eval num_timesteps=318000, episode_reward=0.65 +/- 0.72\n",
      "Episode length: 30.09 +/- 0.53\n",
      "Eval num_timesteps=319000, episode_reward=0.58 +/- 0.79\n",
      "Episode length: 30.05 +/- 0.58\n",
      "Eval num_timesteps=320000, episode_reward=0.59 +/- 0.78\n",
      "Episode length: 30.06 +/- 0.53\n",
      "Eval num_timesteps=321000, episode_reward=0.55 +/- 0.80\n",
      "Episode length: 30.03 +/- 0.52\n",
      "Eval num_timesteps=322000, episode_reward=0.59 +/- 0.79\n",
      "Episode length: 29.98 +/- 1.06\n",
      "Eval num_timesteps=323000, episode_reward=0.55 +/- 0.81\n",
      "Episode length: 30.04 +/- 0.57\n",
      "Eval num_timesteps=324000, episode_reward=0.47 +/- 0.86\n",
      "Episode length: 30.11 +/- 0.60\n",
      "Eval num_timesteps=325000, episode_reward=0.61 +/- 0.78\n",
      "Episode length: 30.04 +/- 0.52\n",
      "Eval num_timesteps=326000, episode_reward=0.52 +/- 0.84\n",
      "Episode length: 30.08 +/- 0.59\n",
      "Eval num_timesteps=327000, episode_reward=0.61 +/- 0.77\n",
      "Episode length: 30.02 +/- 0.56\n",
      "Eval num_timesteps=328000, episode_reward=0.56 +/- 0.81\n",
      "Episode length: 30.03 +/- 0.57\n",
      "Eval num_timesteps=329000, episode_reward=0.61 +/- 0.77\n",
      "Episode length: 30.08 +/- 0.53\n",
      "Eval num_timesteps=330000, episode_reward=0.54 +/- 0.82\n",
      "Episode length: 30.00 +/- 1.25\n",
      "Eval num_timesteps=331000, episode_reward=0.60 +/- 0.79\n",
      "Episode length: 30.06 +/- 0.58\n",
      "Eval num_timesteps=332000, episode_reward=0.58 +/- 0.80\n",
      "Episode length: 30.06 +/- 0.61\n",
      "Eval num_timesteps=333000, episode_reward=0.55 +/- 0.82\n",
      "Episode length: 30.07 +/- 1.26\n",
      "Eval num_timesteps=334000, episode_reward=0.59 +/- 0.79\n",
      "Episode length: 30.07 +/- 1.23\n",
      "Eval num_timesteps=335000, episode_reward=0.58 +/- 0.79\n",
      "Episode length: 30.07 +/- 0.66\n",
      "Eval num_timesteps=336000, episode_reward=0.59 +/- 0.79\n",
      "Episode length: 30.05 +/- 0.56\n",
      "Eval num_timesteps=337000, episode_reward=0.54 +/- 0.82\n",
      "Episode length: 30.09 +/- 0.56\n",
      "Eval num_timesteps=338000, episode_reward=0.61 +/- 0.78\n",
      "Episode length: 30.11 +/- 0.56\n",
      "Eval num_timesteps=339000, episode_reward=0.59 +/- 0.79\n",
      "Episode length: 30.03 +/- 0.57\n",
      "Eval num_timesteps=340000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.03 +/- 1.12\n",
      "Eval num_timesteps=341000, episode_reward=0.61 +/- 0.77\n",
      "Episode length: 30.12 +/- 0.56\n",
      "Eval num_timesteps=342000, episode_reward=0.67 +/- 0.73\n",
      "Episode length: 30.03 +/- 0.93\n",
      "Eval num_timesteps=343000, episode_reward=0.62 +/- 0.76\n",
      "Episode length: 30.13 +/- 0.55\n",
      "Eval num_timesteps=344000, episode_reward=0.54 +/- 0.82\n",
      "Episode length: 30.07 +/- 0.58\n",
      "Eval num_timesteps=345000, episode_reward=0.57 +/- 0.81\n",
      "Episode length: 30.13 +/- 0.56\n",
      "Eval num_timesteps=346000, episode_reward=0.57 +/- 0.80\n",
      "Episode length: 30.08 +/- 0.55\n",
      "Eval num_timesteps=347000, episode_reward=0.57 +/- 0.80\n",
      "Episode length: 30.00 +/- 1.63\n",
      "Eval num_timesteps=348000, episode_reward=0.64 +/- 0.75\n",
      "Episode length: 30.09 +/- 0.54\n",
      "Eval num_timesteps=349000, episode_reward=0.65 +/- 0.73\n",
      "Episode length: 30.08 +/- 0.54\n",
      "Eval num_timesteps=350000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.11 +/- 0.58\n",
      "Eval num_timesteps=351000, episode_reward=0.63 +/- 0.75\n",
      "Episode length: 30.07 +/- 0.53\n",
      "Eval num_timesteps=352000, episode_reward=0.53 +/- 0.83\n",
      "Episode length: 30.05 +/- 0.58\n",
      "Eval num_timesteps=353000, episode_reward=0.61 +/- 0.78\n",
      "Episode length: 30.06 +/- 0.58\n",
      "Eval num_timesteps=354000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.09 +/- 0.56\n",
      "Eval num_timesteps=355000, episode_reward=0.64 +/- 0.75\n",
      "Episode length: 30.10 +/- 0.56\n",
      "Eval num_timesteps=356000, episode_reward=0.65 +/- 0.72\n",
      "Episode length: 30.09 +/- 0.59\n",
      "Eval num_timesteps=357000, episode_reward=0.60 +/- 0.78\n",
      "Episode length: 30.09 +/- 0.56\n",
      "Eval num_timesteps=358000, episode_reward=0.61 +/- 0.77\n",
      "Episode length: 30.09 +/- 0.58\n",
      "Eval num_timesteps=359000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.05 +/- 0.57\n",
      "Eval num_timesteps=360000, episode_reward=0.61 +/- 0.78\n",
      "Episode length: 30.10 +/- 0.55\n",
      "Eval num_timesteps=361000, episode_reward=0.60 +/- 0.78\n",
      "Episode length: 30.11 +/- 0.58\n",
      "Eval num_timesteps=362000, episode_reward=0.60 +/- 0.79\n",
      "Episode length: 30.02 +/- 0.56\n",
      "Eval num_timesteps=363000, episode_reward=0.63 +/- 0.75\n",
      "Episode length: 30.13 +/- 0.53\n",
      "Eval num_timesteps=364000, episode_reward=0.57 +/- 0.80\n",
      "Episode length: 30.07 +/- 0.52\n",
      "Eval num_timesteps=365000, episode_reward=0.61 +/- 0.78\n",
      "Episode length: 30.09 +/- 0.56\n",
      "Eval num_timesteps=366000, episode_reward=0.57 +/- 0.80\n",
      "Episode length: 30.09 +/- 0.58\n",
      "Eval num_timesteps=367000, episode_reward=0.64 +/- 0.75\n",
      "Episode length: 30.08 +/- 0.58\n",
      "Eval num_timesteps=368000, episode_reward=0.55 +/- 0.82\n",
      "Episode length: 30.00 +/- 1.21\n",
      "Eval num_timesteps=369000, episode_reward=0.58 +/- 0.80\n",
      "Episode length: 30.03 +/- 0.57\n",
      "Eval num_timesteps=370000, episode_reward=0.58 +/- 0.79\n",
      "Episode length: 30.06 +/- 0.59\n",
      "Eval num_timesteps=371000, episode_reward=0.55 +/- 0.81\n",
      "Episode length: 30.09 +/- 0.51\n",
      "Eval num_timesteps=372000, episode_reward=0.61 +/- 0.77\n",
      "Episode length: 30.11 +/- 0.54\n",
      "Eval num_timesteps=373000, episode_reward=0.57 +/- 0.81\n",
      "Episode length: 30.06 +/- 0.57\n",
      "Eval num_timesteps=374000, episode_reward=0.58 +/- 0.79\n",
      "Episode length: 30.04 +/- 0.55\n",
      "Eval num_timesteps=375000, episode_reward=0.64 +/- 0.76\n",
      "Episode length: 30.04 +/- 0.54\n",
      "Eval num_timesteps=376000, episode_reward=0.63 +/- 0.75\n",
      "Episode length: 30.06 +/- 0.54\n",
      "Eval num_timesteps=377000, episode_reward=0.61 +/- 0.78\n",
      "Episode length: 30.09 +/- 0.55\n",
      "Eval num_timesteps=378000, episode_reward=0.57 +/- 0.82\n",
      "Episode length: 30.11 +/- 0.58\n",
      "Eval num_timesteps=379000, episode_reward=0.58 +/- 0.80\n",
      "Episode length: 30.04 +/- 0.58\n",
      "Eval num_timesteps=380000, episode_reward=0.59 +/- 0.78\n",
      "Episode length: 30.06 +/- 0.53\n",
      "Eval num_timesteps=381000, episode_reward=0.56 +/- 0.82\n",
      "Episode length: 30.02 +/- 0.55\n",
      "Eval num_timesteps=382000, episode_reward=0.55 +/- 0.81\n",
      "Episode length: 30.04 +/- 0.58\n",
      "Eval num_timesteps=383000, episode_reward=0.53 +/- 0.84\n",
      "Episode length: 30.06 +/- 0.53\n",
      "Eval num_timesteps=384000, episode_reward=0.54 +/- 0.81\n",
      "Episode length: 30.06 +/- 0.54\n",
      "Eval num_timesteps=385000, episode_reward=0.60 +/- 0.78\n",
      "Episode length: 30.06 +/- 0.58\n",
      "Eval num_timesteps=386000, episode_reward=0.64 +/- 0.75\n",
      "Episode length: 30.07 +/- 0.53\n",
      "Eval num_timesteps=387000, episode_reward=0.57 +/- 0.80\n",
      "Episode length: 30.02 +/- 1.08\n",
      "Eval num_timesteps=388000, episode_reward=0.57 +/- 0.81\n",
      "Episode length: 30.08 +/- 0.57\n",
      "Eval num_timesteps=389000, episode_reward=0.54 +/- 0.82\n",
      "Episode length: 30.01 +/- 0.57\n",
      "Eval num_timesteps=390000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.05 +/- 0.55\n",
      "Eval num_timesteps=391000, episode_reward=0.52 +/- 0.84\n",
      "Episode length: 30.04 +/- 0.52\n",
      "Eval num_timesteps=392000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.06 +/- 0.53\n",
      "Eval num_timesteps=393000, episode_reward=0.57 +/- 0.81\n",
      "Episode length: 30.05 +/- 0.57\n",
      "Eval num_timesteps=394000, episode_reward=0.61 +/- 0.78\n",
      "Episode length: 30.07 +/- 0.59\n",
      "Eval num_timesteps=395000, episode_reward=0.58 +/- 0.79\n",
      "Episode length: 30.04 +/- 0.54\n",
      "Eval num_timesteps=396000, episode_reward=0.61 +/- 0.77\n",
      "Episode length: 30.11 +/- 0.59\n",
      "Eval num_timesteps=397000, episode_reward=0.59 +/- 0.78\n",
      "Episode length: 30.06 +/- 0.56\n",
      "Eval num_timesteps=398000, episode_reward=0.64 +/- 0.75\n",
      "Episode length: 30.08 +/- 0.60\n",
      "Eval num_timesteps=399000, episode_reward=0.63 +/- 0.76\n",
      "Episode length: 30.07 +/- 1.22\n",
      "Eval num_timesteps=400000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.07 +/- 0.56\n",
      "Eval num_timesteps=401000, episode_reward=0.61 +/- 0.78\n",
      "Episode length: 30.09 +/- 0.55\n",
      "Eval num_timesteps=402000, episode_reward=0.66 +/- 0.73\n",
      "Episode length: 30.02 +/- 1.17\n",
      "Eval num_timesteps=403000, episode_reward=0.66 +/- 0.72\n",
      "Episode length: 30.13 +/- 0.53\n",
      "Eval num_timesteps=404000, episode_reward=0.66 +/- 0.73\n",
      "Episode length: 30.02 +/- 1.18\n",
      "Eval num_timesteps=405000, episode_reward=0.66 +/- 0.74\n",
      "Episode length: 30.02 +/- 1.10\n",
      "Eval num_timesteps=406000, episode_reward=0.60 +/- 0.79\n",
      "Episode length: 30.02 +/- 0.56\n",
      "Eval num_timesteps=407000, episode_reward=0.60 +/- 0.79\n",
      "Episode length: 30.01 +/- 0.52\n",
      "Eval num_timesteps=408000, episode_reward=0.59 +/- 0.79\n",
      "Episode length: 30.01 +/- 1.25\n",
      "Eval num_timesteps=409000, episode_reward=0.58 +/- 0.79\n",
      "Episode length: 30.06 +/- 0.54\n",
      "Eval num_timesteps=410000, episode_reward=0.58 +/- 0.78\n",
      "Episode length: 30.00 +/- 1.16\n",
      "Eval num_timesteps=411000, episode_reward=0.64 +/- 0.75\n",
      "Episode length: 30.08 +/- 0.53\n",
      "Eval num_timesteps=412000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.07 +/- 0.59\n",
      "Eval num_timesteps=413000, episode_reward=0.61 +/- 0.77\n",
      "Episode length: 30.07 +/- 0.57\n",
      "Eval num_timesteps=414000, episode_reward=0.65 +/- 0.75\n",
      "Episode length: 30.07 +/- 0.52\n",
      "Eval num_timesteps=415000, episode_reward=0.60 +/- 0.78\n",
      "Episode length: 30.05 +/- 0.56\n",
      "Eval num_timesteps=416000, episode_reward=0.68 +/- 0.71\n",
      "Episode length: 30.06 +/- 0.53\n",
      "Eval num_timesteps=417000, episode_reward=0.66 +/- 0.73\n",
      "Episode length: 30.04 +/- 0.59\n",
      "Eval num_timesteps=418000, episode_reward=0.68 +/- 0.71\n",
      "Episode length: 30.04 +/- 0.56\n",
      "Eval num_timesteps=419000, episode_reward=0.61 +/- 0.78\n",
      "Episode length: 30.03 +/- 0.55\n",
      "Eval num_timesteps=420000, episode_reward=0.68 +/- 0.72\n",
      "Episode length: 30.11 +/- 0.55\n",
      "Eval num_timesteps=421000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.13 +/- 0.55\n",
      "Eval num_timesteps=422000, episode_reward=0.64 +/- 0.75\n",
      "Episode length: 30.07 +/- 0.59\n",
      "Eval num_timesteps=423000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.10 +/- 0.58\n",
      "Eval num_timesteps=424000, episode_reward=0.63 +/- 0.75\n",
      "Episode length: 30.07 +/- 0.57\n",
      "Eval num_timesteps=425000, episode_reward=0.70 +/- 0.69\n",
      "Episode length: 30.08 +/- 0.59\n",
      "Eval num_timesteps=426000, episode_reward=0.70 +/- 0.70\n",
      "Episode length: 30.08 +/- 0.57\n",
      "Eval num_timesteps=427000, episode_reward=0.63 +/- 0.75\n",
      "Episode length: 30.07 +/- 0.55\n",
      "Eval num_timesteps=428000, episode_reward=0.67 +/- 0.72\n",
      "Episode length: 30.08 +/- 0.57\n",
      "Eval num_timesteps=429000, episode_reward=0.66 +/- 0.74\n",
      "Episode length: 30.11 +/- 0.57\n",
      "Eval num_timesteps=430000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.09 +/- 0.57\n",
      "Eval num_timesteps=431000, episode_reward=0.66 +/- 0.73\n",
      "Episode length: 30.08 +/- 0.57\n",
      "Eval num_timesteps=432000, episode_reward=0.69 +/- 0.70\n",
      "Episode length: 30.10 +/- 0.63\n",
      "Eval num_timesteps=433000, episode_reward=0.74 +/- 0.66\n",
      "Episode length: 30.13 +/- 0.55\n",
      "New best mean reward!\n",
      "Eval num_timesteps=434000, episode_reward=0.72 +/- 0.67\n",
      "Episode length: 30.12 +/- 0.53\n",
      "Eval num_timesteps=435000, episode_reward=0.67 +/- 0.73\n",
      "Episode length: 30.14 +/- 0.59\n",
      "Eval num_timesteps=436000, episode_reward=0.75 +/- 0.64\n",
      "Episode length: 30.10 +/- 0.55\n",
      "New best mean reward!\n",
      "Eval num_timesteps=437000, episode_reward=0.69 +/- 0.70\n",
      "Episode length: 30.08 +/- 0.57\n",
      "Eval num_timesteps=438000, episode_reward=0.69 +/- 0.71\n",
      "Episode length: 30.12 +/- 0.58\n",
      "Eval num_timesteps=439000, episode_reward=0.68 +/- 0.71\n",
      "Episode length: 30.05 +/- 0.54\n",
      "Eval num_timesteps=440000, episode_reward=0.66 +/- 0.74\n",
      "Episode length: 30.09 +/- 0.58\n",
      "Eval num_timesteps=441000, episode_reward=0.71 +/- 0.69\n",
      "Episode length: 30.10 +/- 0.56\n",
      "Eval num_timesteps=442000, episode_reward=0.69 +/- 0.71\n",
      "Episode length: 30.07 +/- 0.56\n",
      "Eval num_timesteps=443000, episode_reward=0.66 +/- 0.74\n",
      "Episode length: 30.07 +/- 0.61\n",
      "Eval num_timesteps=444000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.13 +/- 0.59\n",
      "Eval num_timesteps=445000, episode_reward=0.57 +/- 0.81\n",
      "Episode length: 30.01 +/- 0.52\n",
      "Eval num_timesteps=446000, episode_reward=0.59 +/- 0.79\n",
      "Episode length: 30.11 +/- 0.59\n",
      "Eval num_timesteps=447000, episode_reward=0.65 +/- 0.75\n",
      "Episode length: 30.08 +/- 0.57\n",
      "Eval num_timesteps=448000, episode_reward=0.67 +/- 0.72\n",
      "Episode length: 30.06 +/- 0.55\n",
      "Eval num_timesteps=449000, episode_reward=0.58 +/- 0.79\n",
      "Episode length: 30.03 +/- 1.15\n",
      "Eval num_timesteps=450000, episode_reward=0.58 +/- 0.79\n",
      "Episode length: 30.01 +/- 0.56\n",
      "Eval num_timesteps=451000, episode_reward=0.63 +/- 0.75\n",
      "Episode length: 30.08 +/- 0.57\n",
      "Eval num_timesteps=452000, episode_reward=0.71 +/- 0.69\n",
      "Episode length: 30.04 +/- 0.55\n",
      "Eval num_timesteps=453000, episode_reward=0.67 +/- 0.73\n",
      "Episode length: 30.06 +/- 0.53\n",
      "Eval num_timesteps=454000, episode_reward=0.68 +/- 0.72\n",
      "Episode length: 30.01 +/- 1.05\n",
      "Eval num_timesteps=455000, episode_reward=0.65 +/- 0.75\n",
      "Episode length: 30.10 +/- 0.57\n",
      "Eval num_timesteps=456000, episode_reward=0.64 +/- 0.76\n",
      "Episode length: 30.05 +/- 0.53\n",
      "Eval num_timesteps=457000, episode_reward=0.63 +/- 0.76\n",
      "Episode length: 30.02 +/- 1.21\n",
      "Eval num_timesteps=458000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.05 +/- 0.52\n",
      "Eval num_timesteps=459000, episode_reward=0.61 +/- 0.78\n",
      "Episode length: 30.06 +/- 1.27\n",
      "Eval num_timesteps=460000, episode_reward=0.67 +/- 0.73\n",
      "Episode length: 30.06 +/- 1.30\n",
      "Eval num_timesteps=461000, episode_reward=0.59 +/- 0.79\n",
      "Episode length: 30.04 +/- 1.30\n",
      "Eval num_timesteps=462000, episode_reward=0.66 +/- 0.72\n",
      "Episode length: 30.09 +/- 0.62\n",
      "Eval num_timesteps=463000, episode_reward=0.68 +/- 0.71\n",
      "Episode length: 30.06 +/- 0.55\n",
      "Eval num_timesteps=464000, episode_reward=0.64 +/- 0.73\n",
      "Episode length: 30.08 +/- 0.57\n",
      "Eval num_timesteps=465000, episode_reward=0.64 +/- 0.74\n",
      "Episode length: 30.01 +/- 1.30\n",
      "Eval num_timesteps=466000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.07 +/- 0.55\n",
      "Eval num_timesteps=467000, episode_reward=0.64 +/- 0.74\n",
      "Episode length: 30.04 +/- 1.17\n",
      "Eval num_timesteps=468000, episode_reward=0.69 +/- 0.70\n",
      "Episode length: 30.06 +/- 0.56\n",
      "Eval num_timesteps=469000, episode_reward=0.64 +/- 0.75\n",
      "Episode length: 30.06 +/- 1.26\n",
      "Eval num_timesteps=470000, episode_reward=0.64 +/- 0.76\n",
      "Episode length: 30.07 +/- 0.54\n",
      "Eval num_timesteps=471000, episode_reward=0.61 +/- 0.77\n",
      "Episode length: 30.05 +/- 0.88\n",
      "Eval num_timesteps=472000, episode_reward=0.62 +/- 0.76\n",
      "Episode length: 29.97 +/- 1.14\n",
      "Eval num_timesteps=473000, episode_reward=0.64 +/- 0.75\n",
      "Episode length: 30.11 +/- 0.55\n",
      "Eval num_timesteps=474000, episode_reward=0.65 +/- 0.75\n",
      "Episode length: 30.12 +/- 0.54\n",
      "Eval num_timesteps=475000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.12 +/- 0.60\n",
      "Eval num_timesteps=476000, episode_reward=0.67 +/- 0.72\n",
      "Episode length: 30.10 +/- 0.55\n",
      "Eval num_timesteps=477000, episode_reward=0.69 +/- 0.70\n",
      "Episode length: 30.11 +/- 0.56\n",
      "Eval num_timesteps=478000, episode_reward=0.66 +/- 0.73\n",
      "Episode length: 30.13 +/- 0.61\n",
      "Eval num_timesteps=479000, episode_reward=0.66 +/- 0.73\n",
      "Episode length: 30.10 +/- 0.59\n",
      "Eval num_timesteps=480000, episode_reward=0.64 +/- 0.74\n",
      "Episode length: 30.06 +/- 0.59\n",
      "Eval num_timesteps=481000, episode_reward=0.56 +/- 0.81\n",
      "Episode length: 30.07 +/- 0.58\n",
      "Eval num_timesteps=482000, episode_reward=0.60 +/- 0.77\n",
      "Episode length: 30.09 +/- 0.53\n",
      "Eval num_timesteps=483000, episode_reward=0.62 +/- 0.76\n",
      "Episode length: 30.08 +/- 0.55\n",
      "Eval num_timesteps=484000, episode_reward=0.67 +/- 0.72\n",
      "Episode length: 30.10 +/- 0.55\n",
      "Eval num_timesteps=485000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.07 +/- 0.56\n",
      "Eval num_timesteps=486000, episode_reward=0.63 +/- 0.76\n",
      "Episode length: 30.08 +/- 0.58\n",
      "Eval num_timesteps=487000, episode_reward=0.56 +/- 0.81\n",
      "Episode length: 30.03 +/- 0.55\n",
      "Eval num_timesteps=488000, episode_reward=0.57 +/- 0.81\n",
      "Episode length: 30.05 +/- 0.55\n",
      "Eval num_timesteps=489000, episode_reward=0.63 +/- 0.75\n",
      "Episode length: 30.03 +/- 0.56\n",
      "Eval num_timesteps=490000, episode_reward=0.71 +/- 0.69\n",
      "Episode length: 30.07 +/- 0.53\n",
      "Eval num_timesteps=491000, episode_reward=0.61 +/- 0.77\n",
      "Episode length: 29.87 +/- 1.99\n",
      "Eval num_timesteps=492000, episode_reward=0.58 +/- 0.79\n",
      "Episode length: 30.08 +/- 0.53\n",
      "Eval num_timesteps=493000, episode_reward=0.69 +/- 0.71\n",
      "Episode length: 30.09 +/- 0.54\n",
      "Eval num_timesteps=494000, episode_reward=0.62 +/- 0.76\n",
      "Episode length: 30.05 +/- 0.56\n",
      "Eval num_timesteps=495000, episode_reward=0.67 +/- 0.72\n",
      "Episode length: 30.08 +/- 0.56\n",
      "Eval num_timesteps=496000, episode_reward=0.64 +/- 0.75\n",
      "Episode length: 30.10 +/- 0.61\n",
      "Eval num_timesteps=497000, episode_reward=0.71 +/- 0.69\n",
      "Episode length: 30.13 +/- 0.54\n",
      "Eval num_timesteps=498000, episode_reward=0.68 +/- 0.71\n",
      "Episode length: 30.09 +/- 0.58\n",
      "Eval num_timesteps=499000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.12 +/- 0.57\n",
      "Eval num_timesteps=500000, episode_reward=0.68 +/- 0.72\n",
      "Episode length: 30.14 +/- 0.57\n",
      "Eval num_timesteps=501000, episode_reward=0.66 +/- 0.73\n",
      "Episode length: 30.04 +/- 1.02\n",
      "Eval num_timesteps=502000, episode_reward=0.66 +/- 0.73\n",
      "Episode length: 30.06 +/- 0.52\n",
      "Eval num_timesteps=503000, episode_reward=0.64 +/- 0.75\n",
      "Episode length: 30.07 +/- 0.56\n",
      "Eval num_timesteps=504000, episode_reward=0.62 +/- 0.75\n",
      "Episode length: 30.07 +/- 0.60\n",
      "Eval num_timesteps=505000, episode_reward=0.64 +/- 0.76\n",
      "Episode length: 30.09 +/- 0.55\n",
      "Eval num_timesteps=506000, episode_reward=0.70 +/- 0.70\n",
      "Episode length: 30.15 +/- 0.56\n",
      "Eval num_timesteps=507000, episode_reward=0.68 +/- 0.73\n",
      "Episode length: 30.12 +/- 0.58\n",
      "Eval num_timesteps=508000, episode_reward=0.67 +/- 0.72\n",
      "Episode length: 30.06 +/- 0.93\n",
      "Eval num_timesteps=509000, episode_reward=0.63 +/- 0.76\n",
      "Episode length: 30.04 +/- 0.55\n",
      "Eval num_timesteps=510000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.10 +/- 0.56\n",
      "Eval num_timesteps=511000, episode_reward=0.63 +/- 0.75\n",
      "Episode length: 30.10 +/- 0.59\n",
      "Eval num_timesteps=512000, episode_reward=0.65 +/- 0.75\n",
      "Episode length: 30.09 +/- 0.55\n",
      "Eval num_timesteps=513000, episode_reward=0.68 +/- 0.72\n",
      "Episode length: 30.00 +/- 1.18\n",
      "Eval num_timesteps=514000, episode_reward=0.61 +/- 0.78\n",
      "Episode length: 30.07 +/- 0.54\n",
      "Eval num_timesteps=515000, episode_reward=0.65 +/- 0.75\n",
      "Episode length: 30.07 +/- 0.54\n",
      "Eval num_timesteps=516000, episode_reward=0.66 +/- 0.74\n",
      "Episode length: 30.07 +/- 0.60\n",
      "Eval num_timesteps=517000, episode_reward=0.59 +/- 0.78\n",
      "Episode length: 30.06 +/- 0.59\n",
      "Eval num_timesteps=518000, episode_reward=0.59 +/- 0.79\n",
      "Episode length: 30.08 +/- 0.53\n",
      "Eval num_timesteps=519000, episode_reward=0.63 +/- 0.76\n",
      "Episode length: 30.08 +/- 0.51\n",
      "Eval num_timesteps=520000, episode_reward=0.64 +/- 0.76\n",
      "Episode length: 30.12 +/- 0.59\n",
      "Eval num_timesteps=521000, episode_reward=0.64 +/- 0.75\n",
      "Episode length: 30.04 +/- 1.20\n",
      "Eval num_timesteps=522000, episode_reward=0.70 +/- 0.69\n",
      "Episode length: 29.99 +/- 1.52\n",
      "Eval num_timesteps=523000, episode_reward=0.64 +/- 0.75\n",
      "Episode length: 30.05 +/- 0.56\n",
      "Eval num_timesteps=524000, episode_reward=0.70 +/- 0.71\n",
      "Episode length: 30.15 +/- 0.53\n",
      "Eval num_timesteps=525000, episode_reward=0.61 +/- 0.78\n",
      "Episode length: 30.05 +/- 1.22\n",
      "Eval num_timesteps=526000, episode_reward=0.67 +/- 0.72\n",
      "Episode length: 30.10 +/- 0.55\n",
      "Eval num_timesteps=527000, episode_reward=0.63 +/- 0.76\n",
      "Episode length: 30.11 +/- 0.55\n",
      "Eval num_timesteps=528000, episode_reward=0.60 +/- 0.78\n",
      "Episode length: 30.10 +/- 0.56\n",
      "Eval num_timesteps=529000, episode_reward=0.63 +/- 0.76\n",
      "Episode length: 30.09 +/- 0.59\n",
      "Eval num_timesteps=530000, episode_reward=0.66 +/- 0.73\n",
      "Episode length: 30.07 +/- 1.20\n",
      "Eval num_timesteps=531000, episode_reward=0.75 +/- 0.65\n",
      "Episode length: 30.12 +/- 0.56\n",
      "Eval num_timesteps=532000, episode_reward=0.63 +/- 0.76\n",
      "Episode length: 30.02 +/- 0.55\n",
      "Eval num_timesteps=533000, episode_reward=0.65 +/- 0.75\n",
      "Episode length: 30.04 +/- 1.47\n",
      "Eval num_timesteps=534000, episode_reward=0.69 +/- 0.70\n",
      "Episode length: 30.05 +/- 0.60\n",
      "Eval num_timesteps=535000, episode_reward=0.63 +/- 0.77\n",
      "Episode length: 30.11 +/- 0.58\n",
      "Eval num_timesteps=536000, episode_reward=0.70 +/- 0.70\n",
      "Episode length: 30.13 +/- 0.54\n",
      "Eval num_timesteps=537000, episode_reward=0.59 +/- 0.78\n",
      "Episode length: 30.05 +/- 1.18\n",
      "Eval num_timesteps=538000, episode_reward=0.64 +/- 0.76\n",
      "Episode length: 29.99 +/- 1.58\n",
      "Eval num_timesteps=539000, episode_reward=0.55 +/- 0.81\n",
      "Episode length: 30.05 +/- 1.07\n",
      "Eval num_timesteps=540000, episode_reward=0.74 +/- 0.66\n",
      "Episode length: 30.14 +/- 0.56\n",
      "Eval num_timesteps=541000, episode_reward=0.63 +/- 0.76\n",
      "Episode length: 30.14 +/- 0.57\n",
      "Eval num_timesteps=542000, episode_reward=0.68 +/- 0.71\n",
      "Episode length: 30.07 +/- 0.57\n",
      "Eval num_timesteps=543000, episode_reward=0.66 +/- 0.73\n",
      "Episode length: 30.13 +/- 1.19\n",
      "Eval num_timesteps=544000, episode_reward=0.64 +/- 0.75\n",
      "Episode length: 30.11 +/- 0.58\n",
      "Eval num_timesteps=545000, episode_reward=0.64 +/- 0.73\n",
      "Episode length: 30.16 +/- 0.58\n",
      "Eval num_timesteps=546000, episode_reward=0.68 +/- 0.72\n",
      "Episode length: 30.14 +/- 0.56\n",
      "Eval num_timesteps=547000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.04 +/- 1.23\n",
      "Eval num_timesteps=548000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.10 +/- 0.58\n",
      "Eval num_timesteps=549000, episode_reward=0.63 +/- 0.75\n",
      "Episode length: 30.05 +/- 1.60\n",
      "Eval num_timesteps=550000, episode_reward=0.64 +/- 0.75\n",
      "Episode length: 30.09 +/- 0.54\n",
      "Eval num_timesteps=551000, episode_reward=0.67 +/- 0.73\n",
      "Episode length: 30.09 +/- 0.59\n",
      "Eval num_timesteps=552000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.08 +/- 1.23\n",
      "Eval num_timesteps=553000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.10 +/- 0.60\n",
      "Eval num_timesteps=554000, episode_reward=0.62 +/- 0.78\n",
      "Episode length: 30.06 +/- 1.09\n",
      "Eval num_timesteps=555000, episode_reward=0.66 +/- 0.73\n",
      "Episode length: 30.11 +/- 0.54\n",
      "Eval num_timesteps=556000, episode_reward=0.59 +/- 0.78\n",
      "Episode length: 30.01 +/- 1.09\n",
      "Eval num_timesteps=557000, episode_reward=0.67 +/- 0.72\n",
      "Episode length: 30.09 +/- 1.16\n",
      "Eval num_timesteps=558000, episode_reward=0.61 +/- 0.78\n",
      "Episode length: 30.07 +/- 1.21\n",
      "Eval num_timesteps=559000, episode_reward=0.63 +/- 0.76\n",
      "Episode length: 30.12 +/- 0.56\n",
      "Eval num_timesteps=560000, episode_reward=0.57 +/- 0.80\n",
      "Episode length: 30.10 +/- 0.54\n",
      "Eval num_timesteps=561000, episode_reward=0.64 +/- 0.74\n",
      "Episode length: 30.13 +/- 0.54\n",
      "Eval num_timesteps=562000, episode_reward=0.61 +/- 0.77\n",
      "Episode length: 30.10 +/- 0.58\n",
      "Eval num_timesteps=563000, episode_reward=0.64 +/- 0.75\n",
      "Episode length: 30.12 +/- 0.58\n",
      "Eval num_timesteps=564000, episode_reward=0.55 +/- 0.81\n",
      "Episode length: 30.11 +/- 1.20\n",
      "Eval num_timesteps=565000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.11 +/- 1.15\n",
      "Eval num_timesteps=566000, episode_reward=0.63 +/- 0.76\n",
      "Episode length: 30.07 +/- 0.59\n",
      "Eval num_timesteps=567000, episode_reward=0.58 +/- 0.78\n",
      "Episode length: 30.05 +/- 1.23\n",
      "Eval num_timesteps=568000, episode_reward=0.53 +/- 0.82\n",
      "Episode length: 30.00 +/- 1.18\n",
      "Eval num_timesteps=569000, episode_reward=0.60 +/- 0.78\n",
      "Episode length: 30.09 +/- 1.18\n",
      "Eval num_timesteps=570000, episode_reward=0.64 +/- 0.75\n",
      "Episode length: 30.09 +/- 0.55\n",
      "Eval num_timesteps=571000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.13 +/- 0.58\n",
      "Eval num_timesteps=572000, episode_reward=0.59 +/- 0.78\n",
      "Episode length: 30.03 +/- 1.10\n",
      "Eval num_timesteps=573000, episode_reward=0.59 +/- 0.80\n",
      "Episode length: 30.06 +/- 1.09\n",
      "Eval num_timesteps=574000, episode_reward=0.57 +/- 0.80\n",
      "Episode length: 30.05 +/- 1.11\n",
      "Eval num_timesteps=575000, episode_reward=0.60 +/- 0.78\n",
      "Episode length: 30.12 +/- 0.54\n",
      "Eval num_timesteps=576000, episode_reward=0.61 +/- 0.78\n",
      "Episode length: 30.07 +/- 1.22\n",
      "Eval num_timesteps=577000, episode_reward=0.54 +/- 0.82\n",
      "Episode length: 30.09 +/- 0.55\n",
      "Eval num_timesteps=578000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.11 +/- 0.57\n",
      "Eval num_timesteps=579000, episode_reward=0.64 +/- 0.74\n",
      "Episode length: 30.06 +/- 1.09\n",
      "Eval num_timesteps=580000, episode_reward=0.60 +/- 0.78\n",
      "Episode length: 30.08 +/- 0.55\n",
      "Eval num_timesteps=581000, episode_reward=0.61 +/- 0.78\n",
      "Episode length: 30.15 +/- 0.62\n",
      "Eval num_timesteps=582000, episode_reward=0.62 +/- 0.76\n",
      "Episode length: 30.09 +/- 0.60\n",
      "Eval num_timesteps=583000, episode_reward=0.55 +/- 0.81\n",
      "Episode length: 30.08 +/- 0.60\n",
      "Eval num_timesteps=584000, episode_reward=0.53 +/- 0.82\n",
      "Episode length: 30.06 +/- 0.59\n",
      "Eval num_timesteps=585000, episode_reward=0.61 +/- 0.78\n",
      "Episode length: 30.05 +/- 0.56\n",
      "Eval num_timesteps=586000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.08 +/- 0.56\n",
      "Eval num_timesteps=587000, episode_reward=0.63 +/- 0.76\n",
      "Episode length: 30.02 +/- 1.23\n",
      "Eval num_timesteps=588000, episode_reward=0.56 +/- 0.81\n",
      "Episode length: 30.09 +/- 0.59\n",
      "Eval num_timesteps=589000, episode_reward=0.58 +/- 0.80\n",
      "Episode length: 29.98 +/- 1.42\n",
      "Eval num_timesteps=590000, episode_reward=0.64 +/- 0.75\n",
      "Episode length: 30.03 +/- 1.19\n",
      "Eval num_timesteps=591000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.09 +/- 0.59\n",
      "Eval num_timesteps=592000, episode_reward=0.61 +/- 0.77\n",
      "Episode length: 29.99 +/- 1.64\n",
      "Eval num_timesteps=593000, episode_reward=0.58 +/- 0.79\n",
      "Episode length: 29.96 +/- 1.56\n",
      "Eval num_timesteps=594000, episode_reward=0.67 +/- 0.71\n",
      "Episode length: 30.16 +/- 0.55\n",
      "Eval num_timesteps=595000, episode_reward=0.58 +/- 0.79\n",
      "Episode length: 30.04 +/- 1.12\n",
      "Eval num_timesteps=596000, episode_reward=0.51 +/- 0.84\n",
      "Episode length: 30.06 +/- 1.03\n",
      "Eval num_timesteps=597000, episode_reward=0.57 +/- 0.80\n",
      "Episode length: 30.10 +/- 0.58\n",
      "Eval num_timesteps=598000, episode_reward=0.53 +/- 0.83\n",
      "Episode length: 30.05 +/- 0.58\n",
      "Eval num_timesteps=599000, episode_reward=0.52 +/- 0.84\n",
      "Episode length: 30.06 +/- 0.58\n",
      "Eval num_timesteps=600000, episode_reward=0.58 +/- 0.79\n",
      "Episode length: 30.05 +/- 0.58\n",
      "Eval num_timesteps=601000, episode_reward=0.55 +/- 0.82\n",
      "Episode length: 30.10 +/- 0.59\n",
      "Eval num_timesteps=602000, episode_reward=0.53 +/- 0.83\n",
      "Episode length: 30.12 +/- 0.57\n",
      "Eval num_timesteps=603000, episode_reward=0.55 +/- 0.82\n",
      "Episode length: 30.11 +/- 0.58\n",
      "Eval num_timesteps=604000, episode_reward=0.56 +/- 0.80\n",
      "Episode length: 30.11 +/- 0.60\n",
      "Eval num_timesteps=605000, episode_reward=0.50 +/- 0.84\n",
      "Episode length: 30.04 +/- 0.55\n",
      "Eval num_timesteps=606000, episode_reward=0.55 +/- 0.81\n",
      "Episode length: 30.12 +/- 0.64\n",
      "Eval num_timesteps=607000, episode_reward=0.55 +/- 0.81\n",
      "Episode length: 30.13 +/- 0.58\n",
      "Eval num_timesteps=608000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.12 +/- 0.57\n",
      "Eval num_timesteps=609000, episode_reward=0.59 +/- 0.78\n",
      "Episode length: 30.11 +/- 0.61\n",
      "Eval num_timesteps=610000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.14 +/- 0.57\n",
      "Eval num_timesteps=611000, episode_reward=0.57 +/- 0.80\n",
      "Episode length: 30.02 +/- 1.15\n",
      "Eval num_timesteps=612000, episode_reward=0.60 +/- 0.78\n",
      "Episode length: 30.06 +/- 0.54\n",
      "Eval num_timesteps=613000, episode_reward=0.63 +/- 0.75\n",
      "Episode length: 30.07 +/- 1.16\n",
      "Eval num_timesteps=614000, episode_reward=0.57 +/- 0.80\n",
      "Episode length: 30.05 +/- 0.57\n",
      "Eval num_timesteps=615000, episode_reward=0.57 +/- 0.80\n",
      "Episode length: 30.10 +/- 0.60\n",
      "Eval num_timesteps=616000, episode_reward=0.60 +/- 0.78\n",
      "Episode length: 30.01 +/- 1.00\n",
      "Eval num_timesteps=617000, episode_reward=0.58 +/- 0.80\n",
      "Episode length: 30.07 +/- 0.63\n",
      "Eval num_timesteps=618000, episode_reward=0.61 +/- 0.77\n",
      "Episode length: 30.06 +/- 1.09\n",
      "Eval num_timesteps=619000, episode_reward=0.64 +/- 0.75\n",
      "Episode length: 30.04 +/- 1.24\n",
      "Eval num_timesteps=620000, episode_reward=0.68 +/- 0.71\n",
      "Episode length: 30.12 +/- 0.61\n",
      "Eval num_timesteps=621000, episode_reward=0.66 +/- 0.73\n",
      "Episode length: 30.10 +/- 0.52\n",
      "Eval num_timesteps=622000, episode_reward=0.67 +/- 0.73\n",
      "Episode length: 30.01 +/- 1.11\n",
      "Eval num_timesteps=623000, episode_reward=0.67 +/- 0.72\n",
      "Episode length: 30.03 +/- 0.99\n",
      "Eval num_timesteps=624000, episode_reward=0.68 +/- 0.73\n",
      "Episode length: 30.10 +/- 0.55\n",
      "Eval num_timesteps=625000, episode_reward=0.68 +/- 0.72\n",
      "Episode length: 30.13 +/- 0.53\n",
      "Eval num_timesteps=626000, episode_reward=0.61 +/- 0.77\n",
      "Episode length: 30.08 +/- 0.53\n",
      "Eval num_timesteps=627000, episode_reward=0.68 +/- 0.72\n",
      "Episode length: 30.07 +/- 1.18\n",
      "Eval num_timesteps=628000, episode_reward=0.57 +/- 0.80\n",
      "Episode length: 30.07 +/- 0.95\n",
      "Eval num_timesteps=629000, episode_reward=0.60 +/- 0.78\n",
      "Episode length: 30.05 +/- 1.02\n",
      "Eval num_timesteps=630000, episode_reward=0.63 +/- 0.76\n",
      "Episode length: 30.02 +/- 1.09\n",
      "Eval num_timesteps=631000, episode_reward=0.69 +/- 0.71\n",
      "Episode length: 30.13 +/- 0.55\n",
      "Eval num_timesteps=632000, episode_reward=0.63 +/- 0.76\n",
      "Episode length: 30.03 +/- 1.21\n",
      "Eval num_timesteps=633000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.07 +/- 1.23\n",
      "Eval num_timesteps=634000, episode_reward=0.67 +/- 0.72\n",
      "Episode length: 30.10 +/- 1.07\n",
      "Eval num_timesteps=635000, episode_reward=0.70 +/- 0.70\n",
      "Episode length: 30.13 +/- 0.59\n",
      "Eval num_timesteps=636000, episode_reward=0.72 +/- 0.67\n",
      "Episode length: 30.03 +/- 1.21\n",
      "Eval num_timesteps=637000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.04 +/- 1.14\n",
      "Eval num_timesteps=638000, episode_reward=0.63 +/- 0.77\n",
      "Episode length: 30.07 +/- 0.60\n",
      "Eval num_timesteps=639000, episode_reward=0.63 +/- 0.75\n",
      "Episode length: 30.05 +/- 1.15\n",
      "Eval num_timesteps=640000, episode_reward=0.59 +/- 0.79\n",
      "Episode length: 30.10 +/- 0.57\n",
      "Eval num_timesteps=641000, episode_reward=0.66 +/- 0.72\n",
      "Episode length: 30.13 +/- 0.54\n",
      "Eval num_timesteps=642000, episode_reward=0.73 +/- 0.66\n",
      "Episode length: 30.11 +/- 0.56\n",
      "Eval num_timesteps=643000, episode_reward=0.63 +/- 0.76\n",
      "Episode length: 30.04 +/- 0.57\n",
      "Eval num_timesteps=644000, episode_reward=0.65 +/- 0.75\n",
      "Episode length: 30.08 +/- 0.56\n",
      "Eval num_timesteps=645000, episode_reward=0.69 +/- 0.71\n",
      "Episode length: 30.09 +/- 1.20\n",
      "Eval num_timesteps=646000, episode_reward=0.68 +/- 0.71\n",
      "Episode length: 30.10 +/- 0.56\n",
      "Eval num_timesteps=647000, episode_reward=0.68 +/- 0.72\n",
      "Episode length: 30.13 +/- 0.57\n",
      "Eval num_timesteps=648000, episode_reward=0.66 +/- 0.73\n",
      "Episode length: 30.10 +/- 0.58\n",
      "Eval num_timesteps=649000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.08 +/- 1.01\n",
      "Eval num_timesteps=650000, episode_reward=0.64 +/- 0.75\n",
      "Episode length: 30.00 +/- 1.17\n",
      "Eval num_timesteps=651000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.09 +/- 0.55\n",
      "Eval num_timesteps=652000, episode_reward=0.61 +/- 0.78\n",
      "Episode length: 30.07 +/- 1.11\n",
      "Eval num_timesteps=653000, episode_reward=0.61 +/- 0.77\n",
      "Episode length: 30.09 +/- 0.57\n",
      "Eval num_timesteps=654000, episode_reward=0.63 +/- 0.77\n",
      "Episode length: 30.06 +/- 1.19\n",
      "Eval num_timesteps=655000, episode_reward=0.72 +/- 0.67\n",
      "Episode length: 30.10 +/- 0.58\n",
      "Eval num_timesteps=656000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.06 +/- 1.17\n",
      "Eval num_timesteps=657000, episode_reward=0.72 +/- 0.67\n",
      "Episode length: 30.11 +/- 0.59\n",
      "Eval num_timesteps=658000, episode_reward=0.61 +/- 0.78\n",
      "Episode length: 30.03 +/- 0.96\n",
      "Eval num_timesteps=659000, episode_reward=0.65 +/- 0.75\n",
      "Episode length: 30.03 +/- 1.29\n",
      "Eval num_timesteps=660000, episode_reward=0.65 +/- 0.75\n",
      "Episode length: 30.09 +/- 0.55\n",
      "Eval num_timesteps=661000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.06 +/- 0.55\n",
      "Eval num_timesteps=662000, episode_reward=0.66 +/- 0.72\n",
      "Episode length: 30.08 +/- 0.59\n",
      "Eval num_timesteps=663000, episode_reward=0.71 +/- 0.69\n",
      "Episode length: 30.10 +/- 0.59\n",
      "Eval num_timesteps=664000, episode_reward=0.71 +/- 0.67\n",
      "Episode length: 30.10 +/- 0.54\n",
      "Eval num_timesteps=665000, episode_reward=0.74 +/- 0.66\n",
      "Episode length: 30.12 +/- 0.62\n",
      "Eval num_timesteps=666000, episode_reward=0.63 +/- 0.76\n",
      "Episode length: 29.99 +/- 1.54\n",
      "Eval num_timesteps=667000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.07 +/- 0.55\n",
      "Eval num_timesteps=668000, episode_reward=0.66 +/- 0.73\n",
      "Episode length: 30.11 +/- 0.57\n",
      "Eval num_timesteps=669000, episode_reward=0.67 +/- 0.72\n",
      "Episode length: 30.13 +/- 0.56\n",
      "Eval num_timesteps=670000, episode_reward=0.65 +/- 0.75\n",
      "Episode length: 30.06 +/- 0.51\n",
      "Eval num_timesteps=671000, episode_reward=0.71 +/- 0.69\n",
      "Episode length: 30.03 +/- 1.16\n",
      "Eval num_timesteps=672000, episode_reward=0.69 +/- 0.69\n",
      "Episode length: 30.10 +/- 0.54\n",
      "Eval num_timesteps=673000, episode_reward=0.71 +/- 0.69\n",
      "Episode length: 30.05 +/- 0.55\n",
      "Eval num_timesteps=674000, episode_reward=0.63 +/- 0.76\n",
      "Episode length: 30.11 +/- 0.54\n",
      "Eval num_timesteps=675000, episode_reward=0.66 +/- 0.74\n",
      "Episode length: 30.11 +/- 0.58\n",
      "Eval num_timesteps=676000, episode_reward=0.64 +/- 0.76\n",
      "Episode length: 30.09 +/- 0.60\n",
      "Eval num_timesteps=677000, episode_reward=0.63 +/- 0.77\n",
      "Episode length: 30.05 +/- 1.24\n",
      "Eval num_timesteps=678000, episode_reward=0.63 +/- 0.75\n",
      "Episode length: 30.04 +/- 0.54\n",
      "Eval num_timesteps=679000, episode_reward=0.67 +/- 0.71\n",
      "Episode length: 30.13 +/- 0.58\n",
      "Eval num_timesteps=680000, episode_reward=0.69 +/- 0.71\n",
      "Episode length: 30.05 +/- 0.51\n",
      "Eval num_timesteps=681000, episode_reward=0.65 +/- 0.75\n",
      "Episode length: 30.04 +/- 0.56\n",
      "Eval num_timesteps=682000, episode_reward=0.68 +/- 0.71\n",
      "Episode length: 30.08 +/- 0.52\n",
      "Eval num_timesteps=683000, episode_reward=0.64 +/- 0.75\n",
      "Episode length: 30.07 +/- 0.58\n",
      "Eval num_timesteps=684000, episode_reward=0.69 +/- 0.70\n",
      "Episode length: 30.08 +/- 0.58\n",
      "Eval num_timesteps=685000, episode_reward=0.68 +/- 0.72\n",
      "Episode length: 30.04 +/- 0.66\n",
      "Eval num_timesteps=686000, episode_reward=0.73 +/- 0.66\n",
      "Episode length: 30.09 +/- 1.20\n",
      "Eval num_timesteps=687000, episode_reward=0.68 +/- 0.72\n",
      "Episode length: 30.08 +/- 0.55\n",
      "Eval num_timesteps=688000, episode_reward=0.63 +/- 0.76\n",
      "Episode length: 30.05 +/- 0.54\n",
      "Eval num_timesteps=689000, episode_reward=0.64 +/- 0.75\n",
      "Episode length: 30.07 +/- 1.07\n",
      "Eval num_timesteps=690000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 29.98 +/- 1.39\n",
      "Eval num_timesteps=691000, episode_reward=0.66 +/- 0.73\n",
      "Episode length: 30.09 +/- 0.99\n",
      "Eval num_timesteps=692000, episode_reward=0.69 +/- 0.70\n",
      "Episode length: 30.06 +/- 0.56\n",
      "Eval num_timesteps=693000, episode_reward=0.62 +/- 0.76\n",
      "Episode length: 30.05 +/- 1.28\n",
      "Eval num_timesteps=694000, episode_reward=0.69 +/- 0.71\n",
      "Episode length: 30.00 +/- 1.67\n",
      "Eval num_timesteps=695000, episode_reward=0.66 +/- 0.74\n",
      "Episode length: 30.04 +/- 1.25\n",
      "Eval num_timesteps=696000, episode_reward=0.73 +/- 0.67\n",
      "Episode length: 30.03 +/- 0.54\n",
      "Eval num_timesteps=697000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.06 +/- 1.10\n",
      "Eval num_timesteps=698000, episode_reward=0.64 +/- 0.76\n",
      "Episode length: 30.02 +/- 0.90\n",
      "Eval num_timesteps=699000, episode_reward=0.70 +/- 0.70\n",
      "Episode length: 30.10 +/- 0.60\n",
      "Eval num_timesteps=700000, episode_reward=0.72 +/- 0.67\n",
      "Episode length: 30.05 +/- 1.07\n",
      "Eval num_timesteps=701000, episode_reward=0.63 +/- 0.77\n",
      "Episode length: 29.99 +/- 1.00\n",
      "Eval num_timesteps=702000, episode_reward=0.64 +/- 0.75\n",
      "Episode length: 30.05 +/- 0.61\n",
      "Eval num_timesteps=703000, episode_reward=0.66 +/- 0.74\n",
      "Episode length: 30.08 +/- 0.53\n",
      "Eval num_timesteps=704000, episode_reward=0.62 +/- 0.76\n",
      "Episode length: 30.02 +/- 0.57\n",
      "Eval num_timesteps=705000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.02 +/- 1.25\n",
      "Eval num_timesteps=706000, episode_reward=0.58 +/- 0.79\n",
      "Episode length: 30.08 +/- 0.56\n",
      "Eval num_timesteps=707000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.10 +/- 0.54\n",
      "Eval num_timesteps=708000, episode_reward=0.68 +/- 0.70\n",
      "Episode length: 30.07 +/- 0.57\n",
      "Eval num_timesteps=709000, episode_reward=0.62 +/- 0.76\n",
      "Episode length: 30.04 +/- 1.13\n",
      "Eval num_timesteps=710000, episode_reward=0.66 +/- 0.74\n",
      "Episode length: 30.11 +/- 0.60\n",
      "Eval num_timesteps=711000, episode_reward=0.64 +/- 0.76\n",
      "Episode length: 30.08 +/- 0.57\n",
      "Eval num_timesteps=712000, episode_reward=0.60 +/- 0.77\n",
      "Episode length: 30.07 +/- 0.59\n",
      "Eval num_timesteps=713000, episode_reward=0.60 +/- 0.78\n",
      "Episode length: 30.05 +/- 0.61\n",
      "Eval num_timesteps=714000, episode_reward=0.68 +/- 0.71\n",
      "Episode length: 30.11 +/- 0.57\n",
      "Eval num_timesteps=715000, episode_reward=0.62 +/- 0.78\n",
      "Episode length: 30.13 +/- 0.58\n",
      "Eval num_timesteps=716000, episode_reward=0.64 +/- 0.75\n",
      "Episode length: 30.06 +/- 0.53\n",
      "Eval num_timesteps=717000, episode_reward=0.69 +/- 0.71\n",
      "Episode length: 30.09 +/- 0.56\n",
      "Eval num_timesteps=718000, episode_reward=0.66 +/- 0.74\n",
      "Episode length: 30.03 +/- 1.10\n",
      "Eval num_timesteps=719000, episode_reward=0.68 +/- 0.72\n",
      "Episode length: 29.94 +/- 1.58\n",
      "Eval num_timesteps=720000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.07 +/- 0.58\n",
      "Eval num_timesteps=721000, episode_reward=0.69 +/- 0.70\n",
      "Episode length: 29.91 +/- 1.79\n",
      "Eval num_timesteps=722000, episode_reward=0.60 +/- 0.78\n",
      "Episode length: 30.03 +/- 0.56\n",
      "Eval num_timesteps=723000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.08 +/- 0.58\n",
      "Eval num_timesteps=724000, episode_reward=0.64 +/- 0.74\n",
      "Episode length: 30.04 +/- 0.58\n",
      "Eval num_timesteps=725000, episode_reward=0.60 +/- 0.79\n",
      "Episode length: 30.07 +/- 1.06\n",
      "Eval num_timesteps=726000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.00 +/- 1.17\n",
      "Eval num_timesteps=727000, episode_reward=0.69 +/- 0.71\n",
      "Episode length: 30.11 +/- 0.53\n",
      "Eval num_timesteps=728000, episode_reward=0.67 +/- 0.73\n",
      "Episode length: 30.08 +/- 0.53\n",
      "Eval num_timesteps=729000, episode_reward=0.70 +/- 0.69\n",
      "Episode length: 30.05 +/- 1.16\n",
      "Eval num_timesteps=730000, episode_reward=0.61 +/- 0.76\n",
      "Episode length: 30.05 +/- 0.55\n",
      "Eval num_timesteps=731000, episode_reward=0.68 +/- 0.72\n",
      "Episode length: 30.07 +/- 0.94\n",
      "Eval num_timesteps=732000, episode_reward=0.61 +/- 0.77\n",
      "Episode length: 30.05 +/- 0.56\n",
      "Eval num_timesteps=733000, episode_reward=0.66 +/- 0.74\n",
      "Episode length: 30.05 +/- 0.57\n",
      "Eval num_timesteps=734000, episode_reward=0.71 +/- 0.69\n",
      "Episode length: 30.08 +/- 0.57\n",
      "Eval num_timesteps=735000, episode_reward=0.68 +/- 0.71\n",
      "Episode length: 29.99 +/- 1.42\n",
      "Eval num_timesteps=736000, episode_reward=0.64 +/- 0.75\n",
      "Episode length: 30.07 +/- 0.60\n",
      "Eval num_timesteps=737000, episode_reward=0.63 +/- 0.76\n",
      "Episode length: 30.02 +/- 1.21\n",
      "Eval num_timesteps=738000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.01 +/- 1.10\n",
      "Eval num_timesteps=739000, episode_reward=0.59 +/- 0.79\n",
      "Episode length: 29.99 +/- 1.23\n",
      "Eval num_timesteps=740000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.01 +/- 1.12\n",
      "Eval num_timesteps=741000, episode_reward=0.69 +/- 0.70\n",
      "Episode length: 30.04 +/- 0.97\n",
      "Eval num_timesteps=742000, episode_reward=0.60 +/- 0.78\n",
      "Episode length: 30.05 +/- 0.59\n",
      "Eval num_timesteps=743000, episode_reward=0.57 +/- 0.80\n",
      "Episode length: 30.06 +/- 1.19\n",
      "Eval num_timesteps=744000, episode_reward=0.62 +/- 0.76\n",
      "Episode length: 30.05 +/- 1.18\n",
      "Eval num_timesteps=745000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.07 +/- 0.96\n",
      "Eval num_timesteps=746000, episode_reward=0.64 +/- 0.76\n",
      "Episode length: 30.02 +/- 0.96\n",
      "Eval num_timesteps=747000, episode_reward=0.62 +/- 0.76\n",
      "Episode length: 29.95 +/- 1.54\n",
      "Eval num_timesteps=748000, episode_reward=0.60 +/- 0.79\n",
      "Episode length: 30.04 +/- 0.55\n",
      "Eval num_timesteps=749000, episode_reward=0.59 +/- 0.80\n",
      "Episode length: 30.01 +/- 0.78\n",
      "Eval num_timesteps=750000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.08 +/- 0.50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-fcdfd34ad521>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meval_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/myenv/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    278\u001b[0m     ) -> \"PPO\":\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         return super(PPO, self).learn(\n\u001b[0m\u001b[1;32m    281\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/myenv/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontinue_training\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/myenv/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;31m# Give access to local variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_locals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/myenv/lib/python3.8/site-packages/stable_baselines3/common/callbacks.py\u001b[0m in \u001b[0;36mon_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_training_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/myenv/lib/python3.8/site-packages/stable_baselines3/common/callbacks.py\u001b[0m in \u001b[0;36m_on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0;31m# Return False (stop training) if at least one callback returns False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/myenv/lib/python3.8/site-packages/stable_baselines3/common/callbacks.py\u001b[0m in \u001b[0;36mon_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_training_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/myenv/lib/python3.8/site-packages/stable_baselines3/common/callbacks.py\u001b[0m in \u001b[0;36m_on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_success_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m             episode_rewards, episode_lengths = evaluate_policy(\n\u001b[0m\u001b[1;32m    364\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/myenv/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py\u001b[0m in \u001b[0;36mevaluate_policy\u001b[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/myenv/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \"\"\"\n\u001b[1;32m    162\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/myenv/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mVecEnvStepReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             obs, self.buf_rews[env_idx], self.buf_dones[env_idx], self.buf_infos[env_idx] = self.envs[env_idx].step(\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             )\n",
      "\u001b[0;32m/opt/conda/envs/myenv/lib/python3.8/site-packages/stable_baselines3/common/monitor.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tried to step environment that needs reset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/notebooks/rl_final_project/multi_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_player_num\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_player_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_player_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mencoded_observation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/notebooks/rl_final_project/multi_env.py\u001b[0m in \u001b[0;36mplay\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_player\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_player_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSelfPlayEnv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/notebooks/rl_final_project/players.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, board)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m#action = sample_valid_actions((board,self.player))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m#print(self.env.get_valid((board,self.player)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mmat_valid_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmat_valid_actions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PASS'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/myenv/lib/python3.8/site-packages/boardgame2/env.py\u001b[0m in \u001b[0;36mget_valid\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mvalid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/myenv/lib/python3.8/site-packages/boardgame2/reversi.py\u001b[0m in \u001b[0;36mis_valid\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                     \u001b[0mxx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myy\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mboard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myy\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEMPTY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/myenv/lib/python3.8/site-packages/boardgame2/env.py\u001b[0m in \u001b[0;36mis_index\u001b[0;34m(board, location)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=int(1e10), callback=[eval_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2c6c75-08c5-4a3a-996c-017c5f0b7242",
   "metadata": {},
   "source": [
    "### Lo testeamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4663594e-87fd-40eb-81a6-529e6504e92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test = ReversiEnv(board_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2deb1ff5-0346-4858-a052-8b7e03996215",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env_test.reset()[0]\n",
    "player = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a0cb9362-5efb-4695-8608-91aa7143339d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  1, -1,  0,  0,  0],\n",
       "       [ 0,  0,  0, -1,  1,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0]], dtype=int8)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145d4a0f-0a78-466d-9011-ccd3cfd94653",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.predict(board[np.newaxis,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3e4224a3-008e-4a6b-9e65-5de57f788535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  1, -1,  0,  0,  0],\n",
       "        [ 0,  0,  0, -1,  1,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0]]], dtype=int8)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_test.reset()[0][np.newaxis,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8b62532a-383f-42b5-a327-b454780c3d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, None)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(env_test.reset()[0][np.newaxis,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c51fe613-76e9-4123-b370-4be52d4032c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "board_shape = 8\n",
    "n_envs = 1\n",
    "env = make_reversi_vec_env(\n",
    "    SelfPlayEnv, n_envs=n_envs,\n",
    "    env_kwargs={\n",
    "        'board_shape': board_shape,\n",
    "        'LocalPlayer': RandomPlayer\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "622d040f-1a4c-4ef5-92d4-a1dc40b1a987",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d2bdb84c-7fd0-4623-aefc-9b4240ddb3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  1., -1.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., -1.,  1.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]]], dtype=float32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c384753b-df6d-45e6-8eda-0ed6ab205bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(obs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6d044bd7-e698-481e-95b4-11a120ac4a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = (env.step(model.predict(obs)[0]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a82edcfc-a385-40e7-aaae-af2027502fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  1., -1.,  1.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., -1.,  1.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]]], dtype=float32)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb2461-08f5-44b5-bb83-0542c42e48ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e9188074-50e2-4560-81b2-010b11ee3e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomActorCriticPolicy(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten()\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential()\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4166818-244e-4521-b189-df26e9e784f6",
   "metadata": {},
   "source": [
    "# CUSTOMCNNfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b57eb224-7cfc-4b11-ad3a-c911eab6d580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c25d2e5a-a4e6-458e-934a-59d37e545e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCNN(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    :param observation_space: (gym.Space)\n",
    "    :param features_dim: (int) Number of features extracted.\n",
    "        This corresponds to the number of unit for the last layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 256):\n",
    "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
    "        # We assume CxHxW images (channels first)\n",
    "        # Re-ordering will be done by pre-preprocessing or wrapper\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with th.no_grad():\n",
    "            n_flatten = self.cnn(\n",
    "                th.as_tensor(observation_space.sample()[None]).float()\n",
    "            ).shape[1]\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        return self.linear(self.cnn(observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2650501e-9fef-4499-8390-694d7294c0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomCNN,\n",
    "    features_extractor_kwargs=dict(features_dim=128),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4c4961-b874-4b67-9b19-0d172e3cc325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
